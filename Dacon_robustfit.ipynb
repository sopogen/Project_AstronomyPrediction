{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/ybigta_sdss_train.csv', index_col=0)\n",
    "test = pd.read_csv('data/ybigta_sdss_test.csv', index_col=0)\n",
    "sample_submission = pd.read_csv('data/ybigta_sdss_sample_submission.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_number = {}\n",
    "for i, column in enumerate(sample_submission.columns):\n",
    "    column_number[column] = i\n",
    "    \n",
    "def to_number(x, dic):\n",
    "    return dic[x]\n",
    "\n",
    "train['type_num'] = train['type'].apply(lambda x: to_number(x, column_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                        type  fiberID  psfMag_u  psfMag_g  psfMag_r  psfMag_i  \\\n",
       "id                                                                             \n",
       "415567                  QSO      106 -0.435873 -0.293006 -0.061893 -0.112365   \n",
       "733874                  QSO      492 -0.580260 -0.415342 -0.221442 -0.113627   \n",
       "1009150                 QSO      388 -0.185381 -0.378678 -0.230361 -0.128357   \n",
       "803041                  QSO      531  0.616548  0.437376  0.608051  0.845455   \n",
       "432241                  QSO      180  0.929133  0.371992  0.167015  0.201739   \n",
       "243530                  QSO       12  0.310785  0.097471  0.213110  0.350142   \n",
       "1478802                 QSO      542 -0.217861 -0.178132 -0.034040  0.168839   \n",
       "233471                  QSO      346 -0.351428 -0.040550  0.039564  0.061180   \n",
       "18370                   QSO      398  0.001218 -0.062852  0.039538  0.067687   \n",
       "1235393                 QSO      517  1.709263  0.822173  0.329555  0.060270   \n",
       "1514081                 QSO      578 -0.256843 -0.030165  0.104105  0.312496   \n",
       "719087                  QSO       93 -0.477750 -0.325098 -0.182283 -0.040262   \n",
       "1538103                 QSO      603 -0.096487 -0.193234 -0.154239 -0.061987   \n",
       "1163065                 QSO       40 -0.039978 -0.056861 -0.177834 -0.203362   \n",
       "78999                   QSO      244  0.111288  0.266663  0.234649  0.280721   \n",
       "189531                  QSO      535  0.251587 -0.057932 -0.147936 -0.097679   \n",
       "397078                  QSO      512  1.882447  0.821525  0.485014  0.488291   \n",
       "1098023                 QSO       37 -0.658552 -0.452921 -0.307849 -0.143954   \n",
       "567228                  QSO      156 -0.371247 -0.325414 -0.015985  0.149435   \n",
       "258841                  QSO      152 -0.455643 -0.334509 -0.286408 -0.178050   \n",
       "1706242                 QSO      570  0.218435  0.327336  0.391359  0.292368   \n",
       "720167                  QSO        1 -0.531438 -0.163673 -0.175202 -0.039840   \n",
       "905283                  QSO      617 -0.514030 -0.202227  0.061342  0.166156   \n",
       "628171                  QSO      516 -0.397240 -0.191033 -0.040094 -0.051404   \n",
       "446658                  QSO      443  0.476725 -0.132630 -0.645482 -1.064859   \n",
       "754559                  QSO      585 -0.474076 -0.251508 -0.111928  0.044772   \n",
       "79654                   QSO      191  0.540187 -0.144798 -0.441685 -0.610263   \n",
       "108121                  QSO      548  0.155817  0.266636  0.306972  0.311887   \n",
       "1362510                 QSO      525 -1.043086 -1.548561 -1.303984 -1.188597   \n",
       "281393                  QSO      368 -0.254104 -0.932025 -1.058599 -1.113734   \n",
       "...                     ...      ...       ...       ...       ...       ...   \n",
       "227617   SERENDIPITY_MANUAL      434 -0.009955  0.362003  0.403051  0.841950   \n",
       "41092    SERENDIPITY_MANUAL      553  1.505810  1.877776  1.203820  1.004479   \n",
       "380572   SERENDIPITY_MANUAL      466 -0.336114  0.168721  0.642357  1.048324   \n",
       "68894    SERENDIPITY_MANUAL      369 -0.664320 -0.136645  0.383889  0.804014   \n",
       "344409   SERENDIPITY_MANUAL       28 -0.249570  0.172276  0.132393  0.287436   \n",
       "347488   SERENDIPITY_MANUAL      270  1.242423  2.427542  1.915171  0.559742   \n",
       "31082    SERENDIPITY_MANUAL      280  0.183434 -0.274591 -0.459674 -0.502835   \n",
       "223228   SERENDIPITY_MANUAL      188  0.532635  0.509553  0.548157  0.649488   \n",
       "223960   SERENDIPITY_MANUAL      186  0.046222  0.264811  0.562237  0.854359   \n",
       "55769    SERENDIPITY_MANUAL      336  0.061314  0.216557  0.678469  0.693245   \n",
       "346498   SERENDIPITY_MANUAL       61 -0.769680 -0.734252 -0.805675 -1.245668   \n",
       "54964    SERENDIPITY_MANUAL      332  0.534214  0.809357  0.573824  1.347418   \n",
       "50607    SERENDIPITY_MANUAL       91  1.172918  2.334231  1.657596  0.390239   \n",
       "228249   SERENDIPITY_MANUAL      362  0.183139  0.560852  0.366959  0.767828   \n",
       "223976   SERENDIPITY_MANUAL      187  0.046222  0.264811  0.562237  0.854359   \n",
       "71797    SERENDIPITY_MANUAL      473  0.026317  0.581414  1.117673  0.910465   \n",
       "15383    SERENDIPITY_MANUAL      233  1.796173  1.648998  0.978230  0.751073   \n",
       "360948   SERENDIPITY_MANUAL      194  1.427083  1.790027  1.024190  0.811063   \n",
       "553727              STAR_PN       88  0.926493  1.136270  1.002526  1.288306   \n",
       "223943              STAR_PN      314 -0.281430 -0.189893  0.143570  0.760619   \n",
       "640130              STAR_PN      129  0.260266  0.712336  0.756201  1.300847   \n",
       "1672448             STAR_PN      167  0.129409  0.588940  0.610933  0.827159   \n",
       "1127462             STAR_PN      402  0.185681  0.447991  0.472803  1.032639   \n",
       "1684303             STAR_PN      387  0.311023  0.325746  0.394433  0.572925   \n",
       "477047              STAR_PN      614  0.210596  0.516710  0.841180  1.881068   \n",
       "378750              STAR_PN      433 -0.162114  0.146019  0.167456  0.689229   \n",
       "771875              STAR_PN      222 -0.287621 -0.161832  0.209320  0.968902   \n",
       "1684304             STAR_PN      364  0.311023  0.325746  0.394433  0.572925   \n",
       "378751              STAR_PN      439 -0.162114  0.146019  0.167456  0.689229   \n",
       "1705384             STAR_PN      637  0.087742  0.000616  0.551267  1.142316   \n",
       "\n",
       "         psfMag_z  fiberMag_u  fiberMag_g  fiberMag_r    ...     petroMag_g  \\\n",
       "id                                                       ...                  \n",
       "415567   0.045592   -0.526241   -0.470618   -0.318998    ...      -0.809223   \n",
       "733874   0.093325   -0.599696   -0.370296   -0.123794    ...      -0.126247   \n",
       "1009150  0.039933   -0.152541   -0.316298   -0.108718    ...      -0.096510   \n",
       "803041   0.852428    0.742320    0.545376    0.766557    ...       0.613879   \n",
       "432241   0.223452    1.036929    0.461316    0.286541    ...       0.573924   \n",
       "243530   0.490394    0.389037    0.190811    0.347953    ...       0.322203   \n",
       "1478802  0.267852   -0.158441   -0.096271    0.099237    ...       0.087087   \n",
       "233471   0.075867   -0.361398   -0.032585    0.009545    ...       0.056686   \n",
       "18370    0.125127   -0.111850   -0.159433   -0.089650    ...      -0.188708   \n",
       "1235393 -0.133726    1.502076    0.857911    0.385963    ...       0.987559   \n",
       "1514081  0.485761   -0.255931    0.054208    0.203124    ...       0.233348   \n",
       "719087   0.171859   -0.428536   -0.230860   -0.053444    ...      -0.066016   \n",
       "1538103  0.162472   -0.069348   -0.136813   -0.037239    ...       0.057734   \n",
       "1163065 -0.073050   -0.301913   -0.468454   -0.458332    ...      -0.792556   \n",
       "78999    0.570807    0.162745    0.303488    0.396124    ...       0.343709   \n",
       "189531   0.174335    0.640705    1.375007   -0.015524    ...       0.238385   \n",
       "397078   0.544531    1.853942    0.933455    0.593079    ...       0.976766   \n",
       "1098023 -0.002747   -0.676603   -0.378369   -0.192140    ...      -0.152560   \n",
       "567228   0.232285   -0.336690   -0.242701    0.131931    ...      -0.069378   \n",
       "258841   0.010995   -0.468392   -0.271590   -0.187647    ...      -0.061428   \n",
       "1706242  0.364947    0.244212    0.382380    0.492846    ...       0.493548   \n",
       "720167  -0.279118   -0.552020   -0.141505   -0.125719    ...      -0.064931   \n",
       "905283   0.252134   -0.537180   -0.158298    0.165004    ...       0.051374   \n",
       "628171   0.105672   -0.410943   -0.156865    0.049911    ...       0.063832   \n",
       "446658  -1.137312    0.551487   -0.032092   -0.526457    ...       0.134940   \n",
       "754559   0.179020   -0.490513   -0.198004   -0.008954    ...       0.013973   \n",
       "79654   -0.401999    0.556548   -0.097356   -0.351123    ...       0.101326   \n",
       "108121   0.381575    0.144440    0.124657    0.165010    ...      -0.073339   \n",
       "1362510 -0.947893   -1.085470   -1.514609   -1.190271    ...      -1.116003   \n",
       "281393  -0.991729   -0.253676   -0.900439   -0.966375    ...      -0.579096   \n",
       "...           ...         ...         ...         ...    ...            ...   \n",
       "227617   0.693256   -0.008738    0.377976    0.452600    ...       0.459780   \n",
       "41092    0.895422    1.419923    1.653358    1.084701    ...       0.996004   \n",
       "380572   1.121866   -0.333368    0.244178    0.753664    ...       0.392839   \n",
       "68894    1.148962   -0.685305   -0.052399    0.512784    ...       0.114027   \n",
       "344409   0.469346   -0.202234    0.263412    0.280303    ...       0.345673   \n",
       "347488  -0.118110    1.284741    2.531991    2.049389    ...       2.002563   \n",
       "31082   -0.410539    0.252336   -0.212839   -0.342348    ...      -0.015404   \n",
       "223228   0.788610    0.537674    0.533225    0.553350    ...       0.707877   \n",
       "223960   1.162007    0.053293    0.310892    0.666023    ...       0.448739   \n",
       "55769    1.012270    0.098163    0.291836    0.770027    ...       0.375438   \n",
       "346498  -1.370614   -0.783939   -0.677812   -0.692110    ...      -0.424301   \n",
       "54964    0.911065    0.644355    0.882086    0.711362    ...       0.909759   \n",
       "50607   -0.386979    1.270027    2.371928    1.953132    ...       2.162904   \n",
       "228249   0.890939    0.222163    0.629469    0.443072    ...       0.695864   \n",
       "223976   1.162007    0.053293    0.310892    0.666023    ...       0.448739   \n",
       "71797    0.920058    0.054643    0.653422    1.237071    ...       0.862098   \n",
       "15383    0.706237    1.867818    1.624647    0.909947    ...       1.360336   \n",
       "360948   0.669755    1.501184    1.654692    0.924569    ...       1.229614   \n",
       "553727   2.444567    0.578118    0.470562    0.599734    ...       1.022790   \n",
       "223943   0.793966   -0.389405   -0.290624    0.031877    ...      -0.382775   \n",
       "640130   0.895459    0.281720    0.738550    0.782213    ...       0.786290   \n",
       "1672448  1.115663    0.201786    0.691701    0.756897    ...       0.780103   \n",
       "1127462  0.685494    0.156244    0.432671    0.434677    ...       0.502760   \n",
       "1684303  0.648308    0.404774    0.402626    0.508301    ...       0.537917   \n",
       "477047   1.448744    0.247037    0.620013    1.005355    ...       0.723403   \n",
       "378750   0.501713   -0.215559    0.118349    0.160016    ...       0.222639   \n",
       "771875   2.444231   -0.351776   -0.211317    0.134857    ...      -0.499534   \n",
       "1684304  0.648308    0.404774    0.402626    0.508301    ...       0.537917   \n",
       "378751   0.501713   -0.215559    0.118349    0.160016    ...       0.222639   \n",
       "1705384  1.247068    0.078947    0.027035    0.597646    ...       0.169764   \n",
       "\n",
       "         petroMag_r  petroMag_i  petroMag_z  modelMag_u  modelMag_g  \\\n",
       "id                                                                    \n",
       "415567    -0.702594   -0.717329   -0.654126   -1.075748   -0.952455   \n",
       "733874     0.071913    0.148203    0.294512   -0.464211   -0.127420   \n",
       "1009150    0.082277    0.146719    0.250467    0.010117   -0.096402   \n",
       "803041     0.794042    0.923190    0.969802    0.966397    0.614955   \n",
       "432241     0.408006    0.412831    0.362005    1.367430    0.561257   \n",
       "243530     0.464872    0.534978    0.611626    0.620836    0.324212   \n",
       "1478802    0.253860    0.390901    0.421809   -0.031234    0.077000   \n",
       "233471     0.032793    0.021607    0.020400   -0.302064    0.033045   \n",
       "18370     -0.090746   -0.089519   -0.043039   -0.148825   -0.162776   \n",
       "1235393    0.617162    0.350048    0.133297    2.500770    0.957369   \n",
       "1514081    0.376061    0.528832    0.582442   -0.082217    0.218563   \n",
       "719087     0.108444    0.207795    0.359756   -0.336404   -0.048570   \n",
       "1538103    0.140636    0.188933    0.440055    0.129071    0.058962   \n",
       "1163065   -0.712207   -0.733724   -0.672713   -0.698859   -0.735467   \n",
       "78999      0.391735    0.347180    0.416720    0.276720    0.342923   \n",
       "189531     0.149255    0.188732    0.377540    0.556090    0.184285   \n",
       "397078     0.695008    0.632529    0.731265    2.477567    0.955991   \n",
       "1098023    0.010545    0.138015    0.217148   -0.553493   -0.159414   \n",
       "567228     0.239878    0.349336    0.372200   -0.211670   -0.057379   \n",
       "258841     0.014645    0.092993    0.178965   -0.323617   -0.062423   \n",
       "1706242    0.601506    0.480214    0.463200    0.488463    0.508119   \n",
       "720167    -0.102674   -0.094159   -0.270297   -0.507370   -0.067089   \n",
       "905283     0.330155    0.407147    0.416992   -0.389088    0.053880   \n",
       "628171     0.237320    0.219026    0.296774   -0.238783    0.062965   \n",
       "446658    -0.275088   -0.583649   -0.693337    0.806137    0.123552   \n",
       "754559     0.174304    0.285445    0.382538   -0.342632    0.010976   \n",
       "79654     -0.104640   -0.202181   -0.118267    0.877497    0.112632   \n",
       "108121    -0.001913   -0.000498    0.004675   -0.011185   -0.067962   \n",
       "1362510   -0.832009   -0.689895   -0.546793   -1.013059   -1.119706   \n",
       "281393    -0.635099   -0.631071   -0.588032   -0.075980   -0.573905   \n",
       "...             ...         ...         ...         ...         ...   \n",
       "227617     0.534872    0.752058    0.665780    0.157272    0.474817   \n",
       "41092      0.585354    0.487705    0.535807    2.043380    1.210387   \n",
       "380572     0.790486    1.063309    1.036466   -0.177790    0.382549   \n",
       "68894      0.596332    0.895441    1.635375   -0.570102    0.114197   \n",
       "344409     0.347429    0.407942    0.530964   -0.072659    0.369191   \n",
       "347488     1.910385    0.765166    0.105140    1.749678    2.292550   \n",
       "31082     -0.118322   -0.150089   -0.110519    0.460272   -0.006600   \n",
       "223228     0.748813    0.796477    0.899146    0.879261    0.690498   \n",
       "223960     0.731358    0.940275    1.060154    0.282026    0.460604   \n",
       "55769      0.764023    0.704020    0.736791    0.237857    0.352615   \n",
       "346498    -0.414827   -0.736902   -0.903367   -0.701417   -0.422072   \n",
       "54964      0.757207    1.246013    0.834800    0.857694    0.909294   \n",
       "50607      1.892939    0.600280   -0.059019    1.728964    2.238796   \n",
       "228249     0.560341    0.875078    0.901606    0.432125    0.698573   \n",
       "223976     0.731358    0.940275    1.060154    0.282026    0.460604   \n",
       "71797      1.102042    1.067381    2.180035    0.205080    0.683361   \n",
       "15383      0.693082    0.408636    0.348252    2.507973    1.299867   \n",
       "360948     0.700778    0.443849    0.363770    1.659255    1.213969   \n",
       "553727     0.964622    1.000853    2.171683    1.469020    1.225010   \n",
       "223943    -0.135004    0.061639    0.111356   -0.609440   -0.444108   \n",
       "640130     0.803081    1.225807    0.910438    0.472168    0.768409   \n",
       "1672448    0.822148    0.944679    1.285444    0.395330    0.754117   \n",
       "1127462    0.490508    0.851376    0.554321    0.302015    0.498605   \n",
       "1684303    0.611097    0.699000    0.763743    0.617519    0.522233   \n",
       "477047     1.051808    1.822726    1.280816    0.489502    0.694281   \n",
       "378750     0.266990    0.606177    0.486733   -0.087636    0.232215   \n",
       "771875    -0.040873    0.609748    2.171649   -0.221190   -0.029859   \n",
       "1684304    0.611097    0.699000    0.763743    0.617519    0.522233   \n",
       "378751     0.266990    0.606177    0.486733   -0.087636    0.232215   \n",
       "1705384    0.617030    0.994764    1.037886    0.246618    0.166401   \n",
       "\n",
       "         modelMag_r  modelMag_i  modelMag_z  type_num  \n",
       "id                                                     \n",
       "415567    -0.802477   -0.819242   -0.726988         8  \n",
       "733874     0.074672    0.154372    0.284371         8  \n",
       "1009150    0.073088    0.146803    0.246988         8  \n",
       "803041     0.784281    0.917421    0.897889         8  \n",
       "432241     0.403609    0.396945    0.384822         8  \n",
       "243530     0.453673    0.529666    0.609172         8  \n",
       "1478802    0.248765    0.379648    0.431982         8  \n",
       "233471     0.061973    0.062108    0.055675         8  \n",
       "18370     -0.083879   -0.077550   -0.020793         8  \n",
       "1235393    0.560270    0.309776    0.107083         8  \n",
       "1514081    0.356515    0.504652    0.594084         8  \n",
       "719087     0.118932    0.221229    0.359717         8  \n",
       "1538103    0.136891    0.198239    0.357588         8  \n",
       "1163065   -0.671302   -0.708251   -0.645336         8  \n",
       "78999      0.377268    0.362663    0.472882         8  \n",
       "189531     0.139979    0.169174    0.355125         8  \n",
       "397078     0.681658    0.633374    0.644088         8  \n",
       "1098023    0.010304    0.135108    0.211797         8  \n",
       "567228     0.250138    0.355029    0.380294         8  \n",
       "258841     0.024060    0.103604    0.216922         8  \n",
       "1706242    0.590269    0.477299    0.492617         8  \n",
       "720167    -0.060188   -0.002972   -0.210184         8  \n",
       "905283     0.317585    0.383682    0.413949         8  \n",
       "628171     0.231297    0.213153    0.293534         8  \n",
       "446658    -0.273768   -0.578082   -0.689389         8  \n",
       "754559     0.170382    0.281196    0.348121         8  \n",
       "79654     -0.102329   -0.197875   -0.107312         8  \n",
       "108121    -0.008176   -0.017011    0.019959         8  \n",
       "1362510   -0.833753   -0.687488   -0.538181         8  \n",
       "281393    -0.628902   -0.624882   -0.572005         8  \n",
       "...             ...         ...         ...       ...  \n",
       "227617     0.534345    0.823887    0.680595        17  \n",
       "41092      0.686637    0.467000    0.419010        17  \n",
       "380572     0.805999    1.065529    1.114765        17  \n",
       "68894      0.585769    0.882815    1.135913        17  \n",
       "344409     0.366416    0.456683    0.563130        17  \n",
       "347488     1.828820    0.683558    0.085368        17  \n",
       "31082     -0.120423   -0.145698   -0.110787        17  \n",
       "223228     0.749913    0.780584    0.855516        17  \n",
       "223960     0.746307    0.924610    1.121786        17  \n",
       "55769      0.764739    0.712878    0.919821        17  \n",
       "346498    -0.416900   -0.734521   -0.879619        17  \n",
       "54964      0.736758    1.285137    0.900797        17  \n",
       "50607      1.675173    0.564004   -0.088517        17  \n",
       "228249     0.552993    0.837926    0.901152        17  \n",
       "223976     0.746307    0.924610    1.121786        17  \n",
       "71797      1.133616    0.908777    0.881777        17  \n",
       "15383      0.697906    0.423636    0.359179        17  \n",
       "360948     0.606443    0.369306    0.277773        17  \n",
       "553727     1.113534    1.260860    2.157311        12  \n",
       "223943    -0.155832    0.204153    0.273354        12  \n",
       "640130     0.801582    1.166463    0.874925        12  \n",
       "1672448    0.785975    0.904626    1.110372        12  \n",
       "1127462    0.497042    0.879539    0.623295        12  \n",
       "1684303    0.600806    0.700993    0.726823        12  \n",
       "477047     0.990848    1.798629    1.363742        12  \n",
       "378750     0.266995    0.637183    0.483200        12  \n",
       "771875     0.310001    0.871259    2.157316        12  \n",
       "1684304    0.600806    0.700993    0.726823        12  \n",
       "378751     0.266995    0.637183    0.483200        12  \n",
       "1705384    0.633082    1.025865    1.080171        12  \n",
       "\n",
       "[171965 rows x 23 columns]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Robust scaling\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "columns = train.columns[2:22]\n",
    "rb_scaler = RobustScaler()\n",
    "train_test = pd.concat((train.iloc[:, 2:22], test.iloc[:, 1:]), axis=0)\n",
    "train_test = pd.DataFrame(rb_scaler.fit_transform(train_test), columns=columns, \n",
    "                          index=(list(train.index) + list(test.index)))\n",
    "\n",
    "train_scaled = train_test.iloc[:len(train.index), :]\n",
    "test_scaled = train_test.iloc[len(train.index):, :]\n",
    "train = pd.concat((train.iloc[:, 0:2], train_scaled, train.iloc[:, 22]), axis=1)\n",
    "test = pd.concat((test.iloc[:, 0], test_scaled), axis=1)\n",
    "\n",
    "train.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id            0\n",
       "type          0\n",
       "fiberID       0\n",
       "psfMag_u      0\n",
       "psfMag_g      0\n",
       "psfMag_r      0\n",
       "psfMag_i      0\n",
       "psfMag_z      0\n",
       "fiberMag_u    0\n",
       "fiberMag_g    0\n",
       "fiberMag_r    0\n",
       "fiberMag_i    0\n",
       "fiberMag_z    0\n",
       "petroMag_u    0\n",
       "petroMag_g    0\n",
       "petroMag_r    0\n",
       "petroMag_i    0\n",
       "petroMag_z    0\n",
       "modelMag_u    0\n",
       "modelMag_g    0\n",
       "modelMag_r    0\n",
       "modelMag_i    0\n",
       "modelMag_z    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "type :\n",
    ">천체의 종류. 종속변수.\n",
    "\n",
    "fiberID :\n",
    ">천체를 관측할 때 사용된 광섬유의 식별번호.\n",
    "\n",
    "u/g/r/i/z :\n",
    "> 관측 파장대. Ultraviolet(u) / Green(g) / Red(r) / Near Infrared(i) / Infrared(z)\n",
    "\n",
    "어쩌구Mag :\n",
    ">https://www.sdss.org/dr12/algorithms/magnitudes/#nmgy\n",
    "\n",
    "fiberMag :\n",
    ">The magnitude measured by the frames pipeline to simulate the flux that would fall into a 3″ fiber in typical seeing. \n",
    "\n",
    "psfMag :\n",
    "> optimal measures of the fluxes of stars.\n",
    "\n",
    "modelMag :\n",
    "> optimal measure of the flux of a galaxy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.fiberID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df.groupby('type').size().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "QSO : \n",
    "> 퀘이사(QUASAR). QUASAR는 지구로부터 수십억 광년 떨어진 거리에 있는 활동성 은하의 핵으로서 태양보다 수 조배나 밝으며 고정되어 있으므로 우주공간상 위치측정의 기준이 된다. 0.05-7의 스펙트럼 적색편이를 보인다.\n",
    "\n",
    "STAR :\n",
    "> 항성. 천구에 붙박혀 있어서 별자리를 기준으로 거의 움직이지 않으며, 점같이 보이는 천체.\n",
    "- STAR_RED_DWARF : 적색 왜성. 초거성 등에 비하여 어둡기 때문에 눈에 잘 띄지 않으나 수는 많아 근거리 항성(恒星)의 약 70%를 차지한다. 질량은 태양의 반 이하이다.\n",
    "- STAR_WHITE_DWARF : 백색 왜성.\n",
    "- STAR_BROWN_DWARF : 갈색 왜성. 행성보다는 크지만 항성보다는 질량이 작고, 가시광선 영역의 빛을 내지 못하는 천체. 즉 질량이 태양의 10분의 1보다 작고, 1000분의 1보다 큰 천체.\n",
    "- STAR_SUB_DWARF : low-luminosity subdwarfs. 준왜성. 광도가 동일한 색인 보통의 왜성보다 약 0.5등급 어두운 왜성. 중원소 원자에 의한 청색에서 보라색 영역에 걸친 선스펙트럼 흡수가 적기 때문에 같은 질량, 같은 연령의 왜성에 비해 색이 푸르게 보이므로 어두운 쪽으로 벗어난 계열을 형성한다.\n",
    "- STAR_BHB : Blue Horizontal-Branch stars. 청색거성 or 쌍성 추정\n",
    "- STAR_CARBON : carbon stars, both dwarf and giant. 탄소별. 항성대기의 화학성분 중 탄소가 산소보다 많은 항성이다. 점근거성가지 단계의 탄소별은 표면온도가 낮지만(3000-2000 K), 반경이 매우커서 밝기가 태양의 5000-10000배에 해당하는 적색거성 또는 적색초거성이다.\n",
    "- STAR_PN : the central stars of planetary nebulae. 행성상성운. 백색왜성 또는 백성왜성으로 진화하는 질량이 작은 별 주변에 형성된 팽창하는 고리 형태의 방출성운을 말한다.\n",
    "- STAR_CATY_VAR : cataclysmic variables. 격변변광성. 밝기가 갑자기 밝아졌다가 원래 밝기로 돌아가는 변광성이다.\n",
    "\n",
    "SERENDIPITY :\n",
    ">An open category of targets used in SDSS-I and SDSS-II whose selection criteria explore different regions of \n",
    "parameter space. These include :\n",
    "- objects lying outside the stellar locus in color space (SERENDIP_RED, SERENDIP_BLUE, SERENDIP_DISTANT)\n",
    "- objects coincident with FIRST sources but fainter than the equivalent in quasar target selection; also not restricted to point sources (SERENDIP_FIRST)\n",
    "- hand-selected targets (SERENDIP_MANUAL)\n",
    "    \n",
    "ROSAT :\n",
    "> For typical values of the ratio of optical to X-ray flux of various classes of Galactic and extragalactic sources, there is an excellent match between the depth of SDSS in the optical and that of the ROSAT All Sky Survey (RASS; voges99) in X-rays. Objects detected in SDSS imaging data are positionally matched against X-ray sources from the RASS catalogs.\n",
    "SDSS objects within the RASS positional error circles (commonly 10-20'' radius) are scrutinized further by target selection algorithms tuned to select likely optical counterparts to the X-ray sources. In decreasing priority order, spectra are obtained for SDSS/RASS coincidences that:\n",
    "- are otherwise bright enough for follow-up spectroscopy (ROSAT_D)\n",
    "\n",
    "SPECTROPHOTO_STD :\n",
    ">spectrophotometric standard stars. 분광측광표준항성. 이미 광도가 알려져 분광측광의 비교측정으로 사용됨.\n",
    "\n",
    "GALAXY :\n",
    "> 외부 은하. \n",
    "\n",
    "SKY : \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "sns.heatmap(data = df.corr(), annot=True, fmt='.2f', linewidths=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VIF Factor</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.111200</td>\n",
       "      <td>fiberID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5050.113990</td>\n",
       "      <td>psfMag_u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.872919</td>\n",
       "      <td>psfMag_g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>7.768348</td>\n",
       "      <td>psfMag_r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7.296348</td>\n",
       "      <td>psfMag_i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>8.574168</td>\n",
       "      <td>psfMag_z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>5032.722935</td>\n",
       "      <td>fiberMag_u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.035935</td>\n",
       "      <td>fiberMag_g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.968382</td>\n",
       "      <td>fiberMag_r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>5.939947</td>\n",
       "      <td>fiberMag_i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>6.819918</td>\n",
       "      <td>fiberMag_z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>4.037860</td>\n",
       "      <td>petroMag_u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>5.742946</td>\n",
       "      <td>petroMag_g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>4.561998</td>\n",
       "      <td>petroMag_r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>5.254086</td>\n",
       "      <td>petroMag_i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>10.453532</td>\n",
       "      <td>petroMag_z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>4.324032</td>\n",
       "      <td>modelMag_u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>5.216910</td>\n",
       "      <td>modelMag_g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>4.938478</td>\n",
       "      <td>modelMag_r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>6.887895</td>\n",
       "      <td>modelMag_i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>8.460385</td>\n",
       "      <td>modelMag_z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     VIF Factor    features\n",
       "0      1.111200     fiberID\n",
       "1   5050.113990    psfMag_u\n",
       "2      6.872919    psfMag_g\n",
       "3      7.768348    psfMag_r\n",
       "4      7.296348    psfMag_i\n",
       "5      8.574168    psfMag_z\n",
       "6   5032.722935  fiberMag_u\n",
       "7      1.035935  fiberMag_g\n",
       "8      2.968382  fiberMag_r\n",
       "9      5.939947  fiberMag_i\n",
       "10     6.819918  fiberMag_z\n",
       "11     4.037860  petroMag_u\n",
       "12     5.742946  petroMag_g\n",
       "13     4.561998  petroMag_r\n",
       "14     5.254086  petroMag_i\n",
       "15    10.453532  petroMag_z\n",
       "16     4.324032  modelMag_u\n",
       "17     5.216910  modelMag_g\n",
       "18     4.938478  modelMag_r\n",
       "19     6.887895  modelMag_i\n",
       "20     8.460385  modelMag_z"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "x_data = df.drop(columns=['id','type'])\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(x_data.values, i) for i in range(x_data.shape[1])]\n",
    "vif[\"features\"] = x_data.columns\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VIF Factor</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.079073</td>\n",
       "      <td>fiberID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.411892</td>\n",
       "      <td>psfMag_g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.841427</td>\n",
       "      <td>psfMag_r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>7.066838</td>\n",
       "      <td>psfMag_i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.909444</td>\n",
       "      <td>psfMag_z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.000863</td>\n",
       "      <td>fiberMag_u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.035578</td>\n",
       "      <td>fiberMag_g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.153277</td>\n",
       "      <td>fiberMag_r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>4.949025</td>\n",
       "      <td>fiberMag_i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>6.744337</td>\n",
       "      <td>fiberMag_z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5.445119</td>\n",
       "      <td>petroMag_g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>4.561822</td>\n",
       "      <td>petroMag_r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>5.117781</td>\n",
       "      <td>petroMag_i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>6.541612</td>\n",
       "      <td>petroMag_z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>3.262934</td>\n",
       "      <td>modelMag_u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>4.297223</td>\n",
       "      <td>modelMag_g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>4.831876</td>\n",
       "      <td>modelMag_r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>5.625479</td>\n",
       "      <td>modelMag_i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>7.654106</td>\n",
       "      <td>modelMag_z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    VIF Factor    features\n",
       "0     1.079073     fiberID\n",
       "1     6.411892    psfMag_g\n",
       "2     6.841427    psfMag_r\n",
       "3     7.066838    psfMag_i\n",
       "4     5.909444    psfMag_z\n",
       "5     1.000863  fiberMag_u\n",
       "6     1.035578  fiberMag_g\n",
       "7     2.153277  fiberMag_r\n",
       "8     4.949025  fiberMag_i\n",
       "9     6.744337  fiberMag_z\n",
       "10    5.445119  petroMag_g\n",
       "11    4.561822  petroMag_r\n",
       "12    5.117781  petroMag_i\n",
       "13    6.541612  petroMag_z\n",
       "14    3.262934  modelMag_u\n",
       "15    4.297223  modelMag_g\n",
       "16    4.831876  modelMag_r\n",
       "17    5.625479  modelMag_i\n",
       "18    7.654106  modelMag_z"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vif = pd.DataFrame()\n",
    "x_data = df.drop(columns=['id','type', 'psfMag_u','petroMag_u'])\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(x_data.values, i) for i in range(x_data.shape[1])]\n",
    "vif[\"features\"] = x_data.columns\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x = train.drop(columns=['type', 'type_num'], axis=1)\n",
    "train_y = train['type_num']\n",
    "test_x = test\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_x, train_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\space\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7314559867617801"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(probability=True)\n",
    "svm.fit(X_train, y_train)\n",
    "svm_pred = svm.predict_proba(X_valid)\n",
    "\n",
    "from sklearn import metrics\n",
    "score = metrics.log_loss(y_valid, svm_pred)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4034741224425686"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators = 1000,\n",
    "                            criterion = 'gini')\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict_proba(X_valid)\n",
    "\n",
    "score = metrics.log_loss(y_valid, rf_pred)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4064338122626459"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators = 1000,\n",
    "                            criterion = 'entropy')\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict_proba(X_valid)\n",
    "\n",
    "score = metrics.log_loss(y_valid, rf_pred)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39356148862845614"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators = 2000,\n",
    "                            criterion = 'gini')\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict_proba(X_valid)\n",
    "\n",
    "score = metrics.log_loss(y_valid, rf_pred)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f0734aadda89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                             criterion = 'gini')\n\u001b[1;32m      5\u001b[0m \u001b[0mrf_3000\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mrf_3000_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrf_3000_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rf' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_3000 = RandomForestClassifier(n_estimators = 3000,\n",
    "                            criterion = 'gini')\n",
    "rf_3000.fit(X_train, y_train)\n",
    "rf_3000_pred = rf.predict_proba(X_valid)\n",
    "\n",
    "score = metrics.log_loss(y_valid, rf_3000_pred)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c88618e7b1e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrf_3000_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf_3000\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrf_3000_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'metrics' is not defined"
     ]
    }
   ],
   "source": [
    "rf_3000_pred = rf_3000.predict_proba(X_valid)\n",
    "score = metrics.log_loss(y_valid, rf_3000_pred)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3911361489463834"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "score = metrics.log_loss(y_valid, rf_3000_pred)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgbm = LGBMClassifier(n_jobs=-1,\n",
    "                      num_leaves=127,\n",
    "                      n_estimators=1250,\n",
    "                      learning_rate=0.001)\n",
    "lgbm.fit(X_train, y_train)\n",
    "lgbm_pred = lgbm.predict_proba(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.821522412512289"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "score = metrics.log_loss(y_valid, lgbm_pred)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lgbm.predict_proba(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 2.9349938\ttotal: 287ms\tremaining: 9m 33s\n",
      "1:\tlearn: 2.9254508\ttotal: 570ms\tremaining: 9m 29s\n",
      "2:\tlearn: 2.9164754\ttotal: 849ms\tremaining: 9m 24s\n",
      "3:\tlearn: 2.9073964\ttotal: 1.16s\tremaining: 9m 37s\n",
      "4:\tlearn: 2.8984996\ttotal: 1.45s\tremaining: 9m 39s\n",
      "5:\tlearn: 2.8896511\ttotal: 1.76s\tremaining: 9m 45s\n",
      "6:\tlearn: 2.8807972\ttotal: 2.11s\tremaining: 10m\n",
      "7:\tlearn: 2.8721458\ttotal: 2.47s\tremaining: 10m 14s\n",
      "8:\tlearn: 2.8635416\ttotal: 2.77s\tremaining: 10m 13s\n",
      "9:\tlearn: 2.8550359\ttotal: 3.1s\tremaining: 10m 17s\n",
      "10:\tlearn: 2.8465270\ttotal: 3.39s\tremaining: 10m 12s\n",
      "11:\tlearn: 2.8382610\ttotal: 3.68s\tremaining: 10m 10s\n",
      "12:\tlearn: 2.8301374\ttotal: 3.97s\tremaining: 10m 6s\n",
      "13:\tlearn: 2.8219768\ttotal: 4.29s\tremaining: 10m 8s\n",
      "14:\tlearn: 2.8139377\ttotal: 4.58s\tremaining: 10m 5s\n",
      "15:\tlearn: 2.8062661\ttotal: 4.9s\tremaining: 10m 7s\n",
      "16:\tlearn: 2.7984188\ttotal: 5.18s\tremaining: 10m 4s\n",
      "17:\tlearn: 2.7906428\ttotal: 5.47s\tremaining: 10m 1s\n",
      "18:\tlearn: 2.7828166\ttotal: 5.77s\tremaining: 10m 1s\n",
      "19:\tlearn: 2.7751850\ttotal: 6.09s\tremaining: 10m 3s\n",
      "20:\tlearn: 2.7675490\ttotal: 6.43s\tremaining: 10m 6s\n",
      "21:\tlearn: 2.7600947\ttotal: 6.78s\tremaining: 10m 9s\n",
      "22:\tlearn: 2.7528307\ttotal: 7.18s\tremaining: 10m 17s\n",
      "23:\tlearn: 2.7455526\ttotal: 7.57s\tremaining: 10m 23s\n",
      "24:\tlearn: 2.7383252\ttotal: 7.92s\tremaining: 10m 25s\n",
      "25:\tlearn: 2.7312356\ttotal: 8.27s\tremaining: 10m 27s\n",
      "26:\tlearn: 2.7242175\ttotal: 8.64s\tremaining: 10m 31s\n",
      "27:\tlearn: 2.7172357\ttotal: 8.97s\tremaining: 10m 31s\n",
      "28:\tlearn: 2.7102370\ttotal: 9.32s\tremaining: 10m 33s\n",
      "29:\tlearn: 2.7032893\ttotal: 9.64s\tremaining: 10m 32s\n",
      "30:\tlearn: 2.6967528\ttotal: 9.97s\tremaining: 10m 33s\n",
      "31:\tlearn: 2.6899591\ttotal: 10.3s\tremaining: 10m 32s\n",
      "32:\tlearn: 2.6832906\ttotal: 10.6s\tremaining: 10m 31s\n",
      "33:\tlearn: 2.6766155\ttotal: 11s\tremaining: 10m 34s\n",
      "34:\tlearn: 2.6700823\ttotal: 11.4s\tremaining: 10m 37s\n",
      "35:\tlearn: 2.6635332\ttotal: 11.7s\tremaining: 10m 38s\n",
      "36:\tlearn: 2.6570321\ttotal: 12s\tremaining: 10m 38s\n",
      "37:\tlearn: 2.6505899\ttotal: 12.4s\tremaining: 10m 37s\n",
      "38:\tlearn: 2.6441742\ttotal: 12.7s\tremaining: 10m 36s\n",
      "39:\tlearn: 2.6379065\ttotal: 13s\tremaining: 10m 36s\n",
      "40:\tlearn: 2.6318767\ttotal: 13.3s\tremaining: 10m 35s\n",
      "41:\tlearn: 2.6257106\ttotal: 13.7s\tremaining: 10m 38s\n",
      "42:\tlearn: 2.6195401\ttotal: 14.1s\tremaining: 10m 40s\n",
      "43:\tlearn: 2.6133729\ttotal: 14.4s\tremaining: 10m 40s\n",
      "44:\tlearn: 2.6073121\ttotal: 14.7s\tremaining: 10m 40s\n",
      "45:\tlearn: 2.6012987\ttotal: 15.1s\tremaining: 10m 40s\n",
      "46:\tlearn: 2.5953687\ttotal: 15.4s\tremaining: 10m 39s\n",
      "47:\tlearn: 2.5895588\ttotal: 15.8s\tremaining: 10m 41s\n",
      "48:\tlearn: 2.5836960\ttotal: 16.2s\tremaining: 10m 44s\n",
      "49:\tlearn: 2.5779706\ttotal: 16.6s\tremaining: 10m 47s\n",
      "50:\tlearn: 2.5721726\ttotal: 17s\tremaining: 10m 49s\n",
      "51:\tlearn: 2.5664074\ttotal: 17.4s\tremaining: 10m 50s\n",
      "52:\tlearn: 2.5607744\ttotal: 17.7s\tremaining: 10m 49s\n",
      "53:\tlearn: 2.5552920\ttotal: 18.1s\tremaining: 10m 50s\n",
      "54:\tlearn: 2.5498087\ttotal: 18.4s\tremaining: 10m 50s\n",
      "55:\tlearn: 2.5442716\ttotal: 18.8s\tremaining: 10m 51s\n",
      "56:\tlearn: 2.5388143\ttotal: 19.1s\tremaining: 10m 51s\n",
      "57:\tlearn: 2.5333131\ttotal: 19.5s\tremaining: 10m 51s\n",
      "58:\tlearn: 2.5278885\ttotal: 19.8s\tremaining: 10m 51s\n",
      "59:\tlearn: 2.5227493\ttotal: 20.1s\tremaining: 10m 51s\n",
      "60:\tlearn: 2.5174278\ttotal: 20.5s\tremaining: 10m 50s\n",
      "61:\tlearn: 2.5121027\ttotal: 20.8s\tremaining: 10m 50s\n",
      "62:\tlearn: 2.5068749\ttotal: 21.1s\tremaining: 10m 49s\n",
      "63:\tlearn: 2.5016417\ttotal: 21.5s\tremaining: 10m 49s\n",
      "64:\tlearn: 2.4966223\ttotal: 21.8s\tremaining: 10m 49s\n",
      "65:\tlearn: 2.4914906\ttotal: 22.2s\tremaining: 10m 49s\n",
      "66:\tlearn: 2.4863741\ttotal: 22.5s\tremaining: 10m 48s\n",
      "67:\tlearn: 2.4813660\ttotal: 22.8s\tremaining: 10m 48s\n",
      "68:\tlearn: 2.4763301\ttotal: 23.1s\tremaining: 10m 47s\n",
      "69:\tlearn: 2.4713917\ttotal: 23.5s\tremaining: 10m 46s\n",
      "70:\tlearn: 2.4664840\ttotal: 23.9s\tremaining: 10m 48s\n",
      "71:\tlearn: 2.4616332\ttotal: 24.3s\tremaining: 10m 49s\n",
      "72:\tlearn: 2.4567752\ttotal: 24.7s\tremaining: 10m 51s\n",
      "73:\tlearn: 2.4520131\ttotal: 25.1s\tremaining: 10m 54s\n",
      "74:\tlearn: 2.4472970\ttotal: 25.5s\tremaining: 10m 55s\n",
      "75:\tlearn: 2.4424744\ttotal: 26s\tremaining: 10m 57s\n",
      "76:\tlearn: 2.4376946\ttotal: 26.4s\tremaining: 10m 58s\n",
      "77:\tlearn: 2.4329691\ttotal: 26.8s\tremaining: 11m\n",
      "78:\tlearn: 2.4283616\ttotal: 27.2s\tremaining: 11m\n",
      "79:\tlearn: 2.4237245\ttotal: 27.6s\tremaining: 11m 2s\n",
      "80:\tlearn: 2.4190827\ttotal: 28.2s\tremaining: 11m 8s\n",
      "81:\tlearn: 2.4145877\ttotal: 28.7s\tremaining: 11m 10s\n",
      "82:\tlearn: 2.4100319\ttotal: 29.1s\tremaining: 11m 11s\n",
      "83:\tlearn: 2.4055145\ttotal: 29.5s\tremaining: 11m 13s\n",
      "84:\tlearn: 2.4009870\ttotal: 30s\tremaining: 11m 16s\n",
      "85:\tlearn: 2.3965230\ttotal: 30.5s\tremaining: 11m 18s\n",
      "86:\tlearn: 2.3921235\ttotal: 31s\tremaining: 11m 20s\n",
      "87:\tlearn: 2.3877125\ttotal: 31.5s\tremaining: 11m 24s\n",
      "88:\tlearn: 2.3833046\ttotal: 32s\tremaining: 11m 26s\n",
      "89:\tlearn: 2.3788807\ttotal: 32.4s\tremaining: 11m 28s\n",
      "90:\tlearn: 2.3745642\ttotal: 32.9s\tremaining: 11m 29s\n",
      "91:\tlearn: 2.3702750\ttotal: 33.3s\tremaining: 11m 30s\n",
      "92:\tlearn: 2.3660301\ttotal: 33.7s\tremaining: 11m 31s\n",
      "93:\tlearn: 2.3617857\ttotal: 34.2s\tremaining: 11m 32s\n",
      "94:\tlearn: 2.3575765\ttotal: 34.7s\tremaining: 11m 36s\n",
      "95:\tlearn: 2.3534218\ttotal: 35.2s\tremaining: 11m 38s\n",
      "96:\tlearn: 2.3492928\ttotal: 35.7s\tremaining: 11m 39s\n",
      "97:\tlearn: 2.3451003\ttotal: 36.1s\tremaining: 11m 41s\n",
      "98:\tlearn: 2.3409514\ttotal: 36.6s\tremaining: 11m 41s\n",
      "99:\tlearn: 2.3368414\ttotal: 37s\tremaining: 11m 42s\n",
      "100:\tlearn: 2.3327082\ttotal: 37.3s\tremaining: 11m 42s\n",
      "101:\tlearn: 2.3286956\ttotal: 37.8s\tremaining: 11m 43s\n",
      "102:\tlearn: 2.3246602\ttotal: 38.2s\tremaining: 11m 43s\n",
      "103:\tlearn: 2.3206412\ttotal: 38.6s\tremaining: 11m 44s\n",
      "104:\tlearn: 2.3166528\ttotal: 39s\tremaining: 11m 44s\n",
      "105:\tlearn: 2.3127199\ttotal: 39.4s\tremaining: 11m 44s\n",
      "106:\tlearn: 2.3087785\ttotal: 39.9s\tremaining: 11m 45s\n",
      "107:\tlearn: 2.3049067\ttotal: 40.5s\tremaining: 11m 49s\n",
      "108:\tlearn: 2.3009929\ttotal: 41.1s\tremaining: 11m 52s\n",
      "109:\tlearn: 2.2972154\ttotal: 41.6s\tremaining: 11m 54s\n",
      "110:\tlearn: 2.2934420\ttotal: 42.1s\tremaining: 11m 57s\n",
      "111:\tlearn: 2.2896524\ttotal: 42.6s\tremaining: 11m 58s\n",
      "112:\tlearn: 2.2858163\ttotal: 43.1s\tremaining: 12m\n",
      "113:\tlearn: 2.2820585\ttotal: 43.6s\tremaining: 12m 2s\n",
      "114:\tlearn: 2.2783042\ttotal: 44.2s\tremaining: 12m 3s\n",
      "115:\tlearn: 2.2745686\ttotal: 44.6s\tremaining: 12m 4s\n",
      "116:\tlearn: 2.2708788\ttotal: 45.1s\tremaining: 12m 6s\n",
      "117:\tlearn: 2.2672026\ttotal: 45.6s\tremaining: 12m 7s\n",
      "118:\tlearn: 2.2634953\ttotal: 46s\tremaining: 12m 7s\n",
      "119:\tlearn: 2.2597441\ttotal: 46.4s\tremaining: 12m 7s\n",
      "120:\tlearn: 2.2560791\ttotal: 46.8s\tremaining: 12m 7s\n",
      "121:\tlearn: 2.2524781\ttotal: 47.2s\tremaining: 12m 6s\n",
      "122:\tlearn: 2.2488529\ttotal: 47.6s\tremaining: 12m 6s\n",
      "123:\tlearn: 2.2452518\ttotal: 48s\tremaining: 12m 6s\n",
      "124:\tlearn: 2.2416643\ttotal: 48.4s\tremaining: 12m 5s\n",
      "125:\tlearn: 2.2381074\ttotal: 48.8s\tremaining: 12m 5s\n",
      "126:\tlearn: 2.2346076\ttotal: 49.2s\tremaining: 12m 5s\n",
      "127:\tlearn: 2.2311308\ttotal: 49.5s\tremaining: 12m 4s\n",
      "128:\tlearn: 2.2276429\ttotal: 49.9s\tremaining: 12m 3s\n",
      "129:\tlearn: 2.2241414\ttotal: 50.3s\tremaining: 12m 2s\n",
      "130:\tlearn: 2.2207149\ttotal: 50.6s\tremaining: 12m 2s\n",
      "131:\tlearn: 2.2172802\ttotal: 51s\tremaining: 12m 2s\n",
      "132:\tlearn: 2.2138823\ttotal: 51.4s\tremaining: 12m 1s\n",
      "133:\tlearn: 2.2104553\ttotal: 51.8s\tremaining: 12m\n",
      "134:\tlearn: 2.2071415\ttotal: 52.2s\tremaining: 12m\n",
      "135:\tlearn: 2.2037814\ttotal: 52.5s\tremaining: 12m\n",
      "136:\tlearn: 2.2004293\ttotal: 52.9s\tremaining: 11m 59s\n",
      "137:\tlearn: 2.1971517\ttotal: 53.3s\tremaining: 11m 59s\n",
      "138:\tlearn: 2.1938953\ttotal: 53.7s\tremaining: 11m 58s\n",
      "139:\tlearn: 2.1905875\ttotal: 54.1s\tremaining: 11m 58s\n",
      "140:\tlearn: 2.1873032\ttotal: 54.4s\tremaining: 11m 57s\n",
      "141:\tlearn: 2.1840617\ttotal: 54.8s\tremaining: 11m 57s\n",
      "142:\tlearn: 2.1808066\ttotal: 55.2s\tremaining: 11m 57s\n",
      "143:\tlearn: 2.1775312\ttotal: 55.6s\tremaining: 11m 56s\n",
      "144:\tlearn: 2.1743111\ttotal: 56s\tremaining: 11m 56s\n",
      "145:\tlearn: 2.1711131\ttotal: 56.4s\tremaining: 11m 56s\n",
      "146:\tlearn: 2.1678927\ttotal: 56.8s\tremaining: 11m 56s\n",
      "147:\tlearn: 2.1646982\ttotal: 57.2s\tremaining: 11m 55s\n",
      "148:\tlearn: 2.1615462\ttotal: 57.6s\tremaining: 11m 55s\n",
      "149:\tlearn: 2.1583796\ttotal: 58s\tremaining: 11m 55s\n",
      "150:\tlearn: 2.1552571\ttotal: 58.4s\tremaining: 11m 55s\n",
      "151:\tlearn: 2.1521061\ttotal: 58.9s\tremaining: 11m 55s\n",
      "152:\tlearn: 2.1490393\ttotal: 59.3s\tremaining: 11m 56s\n",
      "153:\tlearn: 2.1459752\ttotal: 59.8s\tremaining: 11m 56s\n",
      "154:\tlearn: 2.1428749\ttotal: 1m\tremaining: 11m 58s\n",
      "155:\tlearn: 2.1397903\ttotal: 1m\tremaining: 11m 58s\n",
      "156:\tlearn: 2.1367671\ttotal: 1m 1s\tremaining: 11m 57s\n",
      "157:\tlearn: 2.1337455\ttotal: 1m 1s\tremaining: 11m 56s\n",
      "158:\tlearn: 2.1307836\ttotal: 1m 1s\tremaining: 11m 56s\n",
      "159:\tlearn: 2.1277428\ttotal: 1m 2s\tremaining: 11m 55s\n",
      "160:\tlearn: 2.1247509\ttotal: 1m 2s\tremaining: 11m 55s\n",
      "161:\tlearn: 2.1218125\ttotal: 1m 3s\tremaining: 11m 54s\n",
      "162:\tlearn: 2.1188333\ttotal: 1m 3s\tremaining: 11m 54s\n",
      "163:\tlearn: 2.1158734\ttotal: 1m 3s\tremaining: 11m 53s\n",
      "164:\tlearn: 2.1129645\ttotal: 1m 4s\tremaining: 11m 52s\n",
      "165:\tlearn: 2.1100467\ttotal: 1m 4s\tremaining: 11m 52s\n",
      "166:\tlearn: 2.1071850\ttotal: 1m 4s\tremaining: 11m 51s\n",
      "167:\tlearn: 2.1042268\ttotal: 1m 5s\tremaining: 11m 51s\n",
      "168:\tlearn: 2.1013642\ttotal: 1m 5s\tremaining: 11m 50s\n",
      "169:\tlearn: 2.0984654\ttotal: 1m 5s\tremaining: 11m 49s\n",
      "170:\tlearn: 2.0955899\ttotal: 1m 6s\tremaining: 11m 49s\n",
      "171:\tlearn: 2.0927278\ttotal: 1m 6s\tremaining: 11m 49s\n",
      "172:\tlearn: 2.0898966\ttotal: 1m 7s\tremaining: 11m 48s\n",
      "173:\tlearn: 2.0870811\ttotal: 1m 7s\tremaining: 11m 47s\n",
      "174:\tlearn: 2.0842156\ttotal: 1m 7s\tremaining: 11m 47s\n",
      "175:\tlearn: 2.0814106\ttotal: 1m 8s\tremaining: 11m 47s\n",
      "176:\tlearn: 2.0786199\ttotal: 1m 8s\tremaining: 11m 46s\n",
      "177:\tlearn: 2.0759017\ttotal: 1m 9s\tremaining: 11m 46s\n",
      "178:\tlearn: 2.0731997\ttotal: 1m 9s\tremaining: 11m 46s\n",
      "179:\tlearn: 2.0703939\ttotal: 1m 9s\tremaining: 11m 45s\n",
      "180:\tlearn: 2.0676422\ttotal: 1m 10s\tremaining: 11m 45s\n",
      "181:\tlearn: 2.0649075\ttotal: 1m 10s\tremaining: 11m 44s\n",
      "182:\tlearn: 2.0621784\ttotal: 1m 10s\tremaining: 11m 44s\n",
      "183:\tlearn: 2.0594779\ttotal: 1m 11s\tremaining: 11m 44s\n",
      "184:\tlearn: 2.0568176\ttotal: 1m 11s\tremaining: 11m 43s\n",
      "185:\tlearn: 2.0541359\ttotal: 1m 12s\tremaining: 11m 43s\n",
      "186:\tlearn: 2.0514790\ttotal: 1m 12s\tremaining: 11m 43s\n",
      "187:\tlearn: 2.0488564\ttotal: 1m 12s\tremaining: 11m 43s\n",
      "188:\tlearn: 2.0461821\ttotal: 1m 13s\tremaining: 11m 42s\n",
      "189:\tlearn: 2.0435379\ttotal: 1m 13s\tremaining: 11m 42s\n",
      "190:\tlearn: 2.0409232\ttotal: 1m 14s\tremaining: 11m 41s\n",
      "191:\tlearn: 2.0382944\ttotal: 1m 14s\tremaining: 11m 41s\n",
      "192:\tlearn: 2.0357092\ttotal: 1m 14s\tremaining: 11m 41s\n",
      "193:\tlearn: 2.0331062\ttotal: 1m 15s\tremaining: 11m 41s\n",
      "194:\tlearn: 2.0305023\ttotal: 1m 15s\tremaining: 11m 41s\n",
      "195:\tlearn: 2.0279075\ttotal: 1m 16s\tremaining: 11m 42s\n",
      "196:\tlearn: 2.0253377\ttotal: 1m 16s\tremaining: 11m 42s\n",
      "197:\tlearn: 2.0227454\ttotal: 1m 17s\tremaining: 11m 43s\n",
      "198:\tlearn: 2.0202014\ttotal: 1m 17s\tremaining: 11m 43s\n",
      "199:\tlearn: 2.0176858\ttotal: 1m 18s\tremaining: 11m 44s\n",
      "200:\tlearn: 2.0152047\ttotal: 1m 18s\tremaining: 11m 45s\n",
      "201:\tlearn: 2.0126553\ttotal: 1m 19s\tremaining: 11m 44s\n",
      "202:\tlearn: 2.0101493\ttotal: 1m 19s\tremaining: 11m 45s\n",
      "203:\tlearn: 2.0076582\ttotal: 1m 20s\tremaining: 11m 45s\n",
      "204:\tlearn: 2.0051421\ttotal: 1m 20s\tremaining: 11m 45s\n",
      "205:\tlearn: 2.0026781\ttotal: 1m 21s\tremaining: 11m 45s\n",
      "206:\tlearn: 2.0001832\ttotal: 1m 21s\tremaining: 11m 45s\n",
      "207:\tlearn: 1.9977296\ttotal: 1m 22s\tremaining: 11m 46s\n",
      "208:\tlearn: 1.9952761\ttotal: 1m 22s\tremaining: 11m 46s\n",
      "209:\tlearn: 1.9928248\ttotal: 1m 22s\tremaining: 11m 46s\n",
      "210:\tlearn: 1.9904474\ttotal: 1m 23s\tremaining: 11m 46s\n",
      "211:\tlearn: 1.9880349\ttotal: 1m 23s\tremaining: 11m 45s\n",
      "212:\tlearn: 1.9856188\ttotal: 1m 24s\tremaining: 11m 45s\n",
      "213:\tlearn: 1.9832218\ttotal: 1m 24s\tremaining: 11m 45s\n",
      "214:\tlearn: 1.9808363\ttotal: 1m 24s\tremaining: 11m 44s\n",
      "215:\tlearn: 1.9784677\ttotal: 1m 25s\tremaining: 11m 44s\n",
      "216:\tlearn: 1.9761054\ttotal: 1m 25s\tremaining: 11m 44s\n",
      "217:\tlearn: 1.9737330\ttotal: 1m 26s\tremaining: 11m 43s\n",
      "218:\tlearn: 1.9713409\ttotal: 1m 26s\tremaining: 11m 43s\n",
      "219:\tlearn: 1.9690126\ttotal: 1m 26s\tremaining: 11m 42s\n",
      "220:\tlearn: 1.9666734\ttotal: 1m 27s\tremaining: 11m 42s\n",
      "221:\tlearn: 1.9643250\ttotal: 1m 27s\tremaining: 11m 42s\n",
      "222:\tlearn: 1.9620124\ttotal: 1m 28s\tremaining: 11m 42s\n",
      "223:\tlearn: 1.9596874\ttotal: 1m 28s\tremaining: 11m 42s\n",
      "224:\tlearn: 1.9573600\ttotal: 1m 29s\tremaining: 11m 42s\n",
      "225:\tlearn: 1.9550672\ttotal: 1m 29s\tremaining: 11m 44s\n",
      "226:\tlearn: 1.9527737\ttotal: 1m 30s\tremaining: 11m 43s\n",
      "227:\tlearn: 1.9504635\ttotal: 1m 30s\tremaining: 11m 43s\n",
      "228:\tlearn: 1.9482118\ttotal: 1m 30s\tremaining: 11m 43s\n",
      "229:\tlearn: 1.9459532\ttotal: 1m 31s\tremaining: 11m 43s\n",
      "230:\tlearn: 1.9436820\ttotal: 1m 31s\tremaining: 11m 43s\n",
      "231:\tlearn: 1.9414097\ttotal: 1m 32s\tremaining: 11m 43s\n",
      "232:\tlearn: 1.9392067\ttotal: 1m 32s\tremaining: 11m 44s\n",
      "233:\tlearn: 1.9369636\ttotal: 1m 33s\tremaining: 11m 45s\n",
      "234:\tlearn: 1.9347593\ttotal: 1m 33s\tremaining: 11m 45s\n",
      "235:\tlearn: 1.9325327\ttotal: 1m 34s\tremaining: 11m 45s\n",
      "236:\tlearn: 1.9303119\ttotal: 1m 34s\tremaining: 11m 45s\n",
      "237:\tlearn: 1.9281130\ttotal: 1m 35s\tremaining: 11m 45s\n",
      "238:\tlearn: 1.9259089\ttotal: 1m 35s\tremaining: 11m 45s\n",
      "239:\tlearn: 1.9237026\ttotal: 1m 36s\tremaining: 11m 45s\n",
      "240:\tlearn: 1.9214342\ttotal: 1m 36s\tremaining: 11m 45s\n",
      "241:\tlearn: 1.9192648\ttotal: 1m 37s\tremaining: 11m 46s\n",
      "242:\tlearn: 1.9170598\ttotal: 1m 37s\tremaining: 11m 46s\n",
      "243:\tlearn: 1.9148735\ttotal: 1m 38s\tremaining: 11m 45s\n",
      "244:\tlearn: 1.9126910\ttotal: 1m 38s\tremaining: 11m 45s\n",
      "245:\tlearn: 1.9105155\ttotal: 1m 38s\tremaining: 11m 44s\n",
      "246:\tlearn: 1.9083634\ttotal: 1m 39s\tremaining: 11m 43s\n",
      "247:\tlearn: 1.9062354\ttotal: 1m 39s\tremaining: 11m 43s\n",
      "248:\tlearn: 1.9040886\ttotal: 1m 39s\tremaining: 11m 43s\n",
      "249:\tlearn: 1.9019786\ttotal: 1m 40s\tremaining: 11m 42s\n",
      "250:\tlearn: 1.8998400\ttotal: 1m 40s\tremaining: 11m 42s\n",
      "251:\tlearn: 1.8976698\ttotal: 1m 41s\tremaining: 11m 41s\n",
      "252:\tlearn: 1.8956171\ttotal: 1m 41s\tremaining: 11m 41s\n",
      "253:\tlearn: 1.8935482\ttotal: 1m 41s\tremaining: 11m 40s\n",
      "254:\tlearn: 1.8914382\ttotal: 1m 42s\tremaining: 11m 40s\n",
      "255:\tlearn: 1.8893769\ttotal: 1m 42s\tremaining: 11m 39s\n",
      "256:\tlearn: 1.8873168\ttotal: 1m 43s\tremaining: 11m 39s\n",
      "257:\tlearn: 1.8852575\ttotal: 1m 43s\tremaining: 11m 38s\n",
      "258:\tlearn: 1.8832176\ttotal: 1m 43s\tremaining: 11m 38s\n",
      "259:\tlearn: 1.8811284\ttotal: 1m 44s\tremaining: 11m 38s\n",
      "260:\tlearn: 1.8790769\ttotal: 1m 44s\tremaining: 11m 37s\n",
      "261:\tlearn: 1.8770398\ttotal: 1m 45s\tremaining: 11m 37s\n",
      "262:\tlearn: 1.8749540\ttotal: 1m 45s\tremaining: 11m 37s\n",
      "263:\tlearn: 1.8729025\ttotal: 1m 45s\tremaining: 11m 36s\n",
      "264:\tlearn: 1.8708663\ttotal: 1m 46s\tremaining: 11m 36s\n",
      "265:\tlearn: 1.8688560\ttotal: 1m 46s\tremaining: 11m 35s\n",
      "266:\tlearn: 1.8668357\ttotal: 1m 47s\tremaining: 11m 35s\n",
      "267:\tlearn: 1.8648332\ttotal: 1m 47s\tremaining: 11m 35s\n",
      "268:\tlearn: 1.8628851\ttotal: 1m 48s\tremaining: 11m 35s\n",
      "269:\tlearn: 1.8608860\ttotal: 1m 48s\tremaining: 11m 34s\n",
      "270:\tlearn: 1.8589040\ttotal: 1m 48s\tremaining: 11m 34s\n",
      "271:\tlearn: 1.8569485\ttotal: 1m 49s\tremaining: 11m 33s\n",
      "272:\tlearn: 1.8549679\ttotal: 1m 49s\tremaining: 11m 32s\n",
      "273:\tlearn: 1.8530109\ttotal: 1m 49s\tremaining: 11m 32s\n",
      "274:\tlearn: 1.8510415\ttotal: 1m 50s\tremaining: 11m 32s\n",
      "275:\tlearn: 1.8491340\ttotal: 1m 50s\tremaining: 11m 31s\n",
      "276:\tlearn: 1.8471611\ttotal: 1m 51s\tremaining: 11m 31s\n",
      "277:\tlearn: 1.8452180\ttotal: 1m 51s\tremaining: 11m 30s\n",
      "278:\tlearn: 1.8432798\ttotal: 1m 51s\tremaining: 11m 30s\n",
      "279:\tlearn: 1.8413735\ttotal: 1m 52s\tremaining: 11m 29s\n",
      "280:\tlearn: 1.8394720\ttotal: 1m 52s\tremaining: 11m 29s\n",
      "281:\tlearn: 1.8375693\ttotal: 1m 53s\tremaining: 11m 28s\n",
      "282:\tlearn: 1.8356421\ttotal: 1m 53s\tremaining: 11m 28s\n",
      "283:\tlearn: 1.8337444\ttotal: 1m 53s\tremaining: 11m 27s\n",
      "284:\tlearn: 1.8318412\ttotal: 1m 54s\tremaining: 11m 27s\n",
      "285:\tlearn: 1.8299693\ttotal: 1m 54s\tremaining: 11m 27s\n",
      "286:\tlearn: 1.8280778\ttotal: 1m 55s\tremaining: 11m 27s\n",
      "287:\tlearn: 1.8262071\ttotal: 1m 55s\tremaining: 11m 27s\n",
      "288:\tlearn: 1.8243378\ttotal: 1m 56s\tremaining: 11m 27s\n",
      "289:\tlearn: 1.8224676\ttotal: 1m 56s\tremaining: 11m 27s\n",
      "290:\tlearn: 1.8206113\ttotal: 1m 57s\tremaining: 11m 27s\n",
      "291:\tlearn: 1.8187401\ttotal: 1m 57s\tremaining: 11m 27s\n",
      "292:\tlearn: 1.8169103\ttotal: 1m 57s\tremaining: 11m 27s\n",
      "293:\tlearn: 1.8150875\ttotal: 1m 58s\tremaining: 11m 27s\n",
      "294:\tlearn: 1.8132434\ttotal: 1m 58s\tremaining: 11m 27s\n",
      "295:\tlearn: 1.8114193\ttotal: 1m 59s\tremaining: 11m 27s\n",
      "296:\tlearn: 1.8095466\ttotal: 1m 59s\tremaining: 11m 26s\n",
      "297:\tlearn: 1.8077357\ttotal: 2m\tremaining: 11m 26s\n",
      "298:\tlearn: 1.8059053\ttotal: 2m\tremaining: 11m 26s\n",
      "299:\tlearn: 1.8040535\ttotal: 2m 1s\tremaining: 11m 26s\n",
      "300:\tlearn: 1.8022343\ttotal: 2m 1s\tremaining: 11m 25s\n",
      "301:\tlearn: 1.8004512\ttotal: 2m 1s\tremaining: 11m 25s\n",
      "302:\tlearn: 1.7986588\ttotal: 2m 2s\tremaining: 11m 25s\n",
      "303:\tlearn: 1.7968898\ttotal: 2m 2s\tremaining: 11m 25s\n",
      "304:\tlearn: 1.7950938\ttotal: 2m 3s\tremaining: 11m 25s\n",
      "305:\tlearn: 1.7933402\ttotal: 2m 3s\tremaining: 11m 25s\n",
      "306:\tlearn: 1.7915454\ttotal: 2m 4s\tremaining: 11m 25s\n",
      "307:\tlearn: 1.7897774\ttotal: 2m 4s\tremaining: 11m 25s\n",
      "308:\tlearn: 1.7880043\ttotal: 2m 5s\tremaining: 11m 25s\n",
      "309:\tlearn: 1.7862093\ttotal: 2m 5s\tremaining: 11m 25s\n",
      "310:\tlearn: 1.7844323\ttotal: 2m 6s\tremaining: 11m 25s\n",
      "311:\tlearn: 1.7827017\ttotal: 2m 6s\tremaining: 11m 24s\n",
      "312:\tlearn: 1.7809716\ttotal: 2m 7s\tremaining: 11m 24s\n",
      "313:\tlearn: 1.7792420\ttotal: 2m 7s\tremaining: 11m 24s\n",
      "314:\tlearn: 1.7775053\ttotal: 2m 7s\tremaining: 11m 23s\n",
      "315:\tlearn: 1.7757955\ttotal: 2m 8s\tremaining: 11m 23s\n",
      "316:\tlearn: 1.7740990\ttotal: 2m 8s\tremaining: 11m 22s\n",
      "317:\tlearn: 1.7723995\ttotal: 2m 8s\tremaining: 11m 22s\n",
      "318:\tlearn: 1.7706924\ttotal: 2m 9s\tremaining: 11m 21s\n",
      "319:\tlearn: 1.7689599\ttotal: 2m 9s\tremaining: 11m 21s\n",
      "320:\tlearn: 1.7672958\ttotal: 2m 10s\tremaining: 11m 20s\n",
      "321:\tlearn: 1.7656356\ttotal: 2m 10s\tremaining: 11m 20s\n",
      "322:\tlearn: 1.7639308\ttotal: 2m 11s\tremaining: 11m 20s\n",
      "323:\tlearn: 1.7622575\ttotal: 2m 11s\tremaining: 11m 20s\n",
      "324:\tlearn: 1.7606013\ttotal: 2m 11s\tremaining: 11m 19s\n",
      "325:\tlearn: 1.7589725\ttotal: 2m 12s\tremaining: 11m 19s\n",
      "326:\tlearn: 1.7573182\ttotal: 2m 12s\tremaining: 11m 18s\n",
      "327:\tlearn: 1.7556608\ttotal: 2m 13s\tremaining: 11m 18s\n",
      "328:\tlearn: 1.7539972\ttotal: 2m 13s\tremaining: 11m 18s\n",
      "329:\tlearn: 1.7523388\ttotal: 2m 13s\tremaining: 11m 17s\n",
      "330:\tlearn: 1.7507156\ttotal: 2m 14s\tremaining: 11m 16s\n",
      "331:\tlearn: 1.7490564\ttotal: 2m 14s\tremaining: 11m 16s\n",
      "332:\tlearn: 1.7474249\ttotal: 2m 15s\tremaining: 11m 15s\n",
      "333:\tlearn: 1.7457833\ttotal: 2m 15s\tremaining: 11m 15s\n",
      "334:\tlearn: 1.7441804\ttotal: 2m 15s\tremaining: 11m 14s\n",
      "335:\tlearn: 1.7425304\ttotal: 2m 16s\tremaining: 11m 14s\n",
      "336:\tlearn: 1.7409133\ttotal: 2m 16s\tremaining: 11m 14s\n",
      "337:\tlearn: 1.7392699\ttotal: 2m 17s\tremaining: 11m 14s\n",
      "338:\tlearn: 1.7376554\ttotal: 2m 17s\tremaining: 11m 14s\n",
      "339:\tlearn: 1.7360639\ttotal: 2m 17s\tremaining: 11m 13s\n",
      "340:\tlearn: 1.7344635\ttotal: 2m 18s\tremaining: 11m 13s\n",
      "341:\tlearn: 1.7328312\ttotal: 2m 18s\tremaining: 11m 13s\n",
      "342:\tlearn: 1.7312529\ttotal: 2m 19s\tremaining: 11m 13s\n",
      "343:\tlearn: 1.7296603\ttotal: 2m 19s\tremaining: 11m 13s\n",
      "344:\tlearn: 1.7280742\ttotal: 2m 20s\tremaining: 11m 13s\n",
      "345:\tlearn: 1.7265056\ttotal: 2m 20s\tremaining: 11m 13s\n",
      "346:\tlearn: 1.7249398\ttotal: 2m 21s\tremaining: 11m 13s\n",
      "347:\tlearn: 1.7233649\ttotal: 2m 21s\tremaining: 11m 12s\n",
      "348:\tlearn: 1.7217702\ttotal: 2m 22s\tremaining: 11m 12s\n",
      "349:\tlearn: 1.7201803\ttotal: 2m 22s\tremaining: 11m 11s\n",
      "350:\tlearn: 1.7186178\ttotal: 2m 22s\tremaining: 11m 11s\n",
      "351:\tlearn: 1.7170618\ttotal: 2m 23s\tremaining: 11m 10s\n",
      "352:\tlearn: 1.7154970\ttotal: 2m 23s\tremaining: 11m 10s\n",
      "353:\tlearn: 1.7139705\ttotal: 2m 24s\tremaining: 11m 9s\n",
      "354:\tlearn: 1.7123947\ttotal: 2m 24s\tremaining: 11m 8s\n",
      "355:\tlearn: 1.7108520\ttotal: 2m 24s\tremaining: 11m 8s\n",
      "356:\tlearn: 1.7093299\ttotal: 2m 25s\tremaining: 11m 7s\n",
      "357:\tlearn: 1.7078070\ttotal: 2m 25s\tremaining: 11m 7s\n",
      "358:\tlearn: 1.7062550\ttotal: 2m 25s\tremaining: 11m 7s\n",
      "359:\tlearn: 1.7047258\ttotal: 2m 26s\tremaining: 11m 6s\n",
      "360:\tlearn: 1.7031092\ttotal: 2m 26s\tremaining: 11m 5s\n",
      "361:\tlearn: 1.7015831\ttotal: 2m 27s\tremaining: 11m 5s\n",
      "362:\tlearn: 1.7000926\ttotal: 2m 27s\tremaining: 11m 4s\n",
      "363:\tlearn: 1.6985428\ttotal: 2m 27s\tremaining: 11m 4s\n",
      "364:\tlearn: 1.6970301\ttotal: 2m 28s\tremaining: 11m 3s\n",
      "365:\tlearn: 1.6955284\ttotal: 2m 28s\tremaining: 11m 3s\n",
      "366:\tlearn: 1.6939530\ttotal: 2m 28s\tremaining: 11m 2s\n",
      "367:\tlearn: 1.6924758\ttotal: 2m 29s\tremaining: 11m 2s\n",
      "368:\tlearn: 1.6909819\ttotal: 2m 29s\tremaining: 11m 2s\n",
      "369:\tlearn: 1.6894192\ttotal: 2m 30s\tremaining: 11m 1s\n",
      "370:\tlearn: 1.6879213\ttotal: 2m 30s\tremaining: 11m 1s\n",
      "371:\tlearn: 1.6864353\ttotal: 2m 30s\tremaining: 11m\n",
      "372:\tlearn: 1.6849217\ttotal: 2m 31s\tremaining: 11m\n",
      "373:\tlearn: 1.6834451\ttotal: 2m 31s\tremaining: 10m 59s\n",
      "374:\tlearn: 1.6818880\ttotal: 2m 32s\tremaining: 10m 59s\n",
      "375:\tlearn: 1.6804214\ttotal: 2m 32s\tremaining: 10m 58s\n",
      "376:\tlearn: 1.6789639\ttotal: 2m 32s\tremaining: 10m 58s\n",
      "377:\tlearn: 1.6775194\ttotal: 2m 33s\tremaining: 10m 57s\n",
      "378:\tlearn: 1.6760085\ttotal: 2m 33s\tremaining: 10m 57s\n",
      "379:\tlearn: 1.6745824\ttotal: 2m 34s\tremaining: 10m 56s\n",
      "380:\tlearn: 1.6731019\ttotal: 2m 34s\tremaining: 10m 56s\n",
      "381:\tlearn: 1.6716608\ttotal: 2m 34s\tremaining: 10m 55s\n",
      "382:\tlearn: 1.6702162\ttotal: 2m 35s\tremaining: 10m 55s\n",
      "383:\tlearn: 1.6687984\ttotal: 2m 35s\tremaining: 10m 55s\n",
      "384:\tlearn: 1.6673258\ttotal: 2m 36s\tremaining: 10m 55s\n",
      "385:\tlearn: 1.6658888\ttotal: 2m 36s\tremaining: 10m 55s\n",
      "386:\tlearn: 1.6644624\ttotal: 2m 37s\tremaining: 10m 54s\n",
      "387:\tlearn: 1.6630369\ttotal: 2m 37s\tremaining: 10m 53s\n",
      "388:\tlearn: 1.6615443\ttotal: 2m 37s\tremaining: 10m 53s\n",
      "389:\tlearn: 1.6601273\ttotal: 2m 38s\tremaining: 10m 52s\n",
      "390:\tlearn: 1.6587233\ttotal: 2m 38s\tremaining: 10m 52s\n",
      "391:\tlearn: 1.6573213\ttotal: 2m 38s\tremaining: 10m 51s\n",
      "392:\tlearn: 1.6559237\ttotal: 2m 39s\tremaining: 10m 51s\n",
      "393:\tlearn: 1.6545347\ttotal: 2m 39s\tremaining: 10m 51s\n",
      "394:\tlearn: 1.6531542\ttotal: 2m 40s\tremaining: 10m 50s\n",
      "395:\tlearn: 1.6517855\ttotal: 2m 40s\tremaining: 10m 49s\n",
      "396:\tlearn: 1.6503853\ttotal: 2m 40s\tremaining: 10m 49s\n",
      "397:\tlearn: 1.6489879\ttotal: 2m 41s\tremaining: 10m 48s\n",
      "398:\tlearn: 1.6476007\ttotal: 2m 41s\tremaining: 10m 48s\n",
      "399:\tlearn: 1.6462419\ttotal: 2m 41s\tremaining: 10m 47s\n",
      "400:\tlearn: 1.6448716\ttotal: 2m 42s\tremaining: 10m 47s\n",
      "401:\tlearn: 1.6434969\ttotal: 2m 42s\tremaining: 10m 47s\n",
      "402:\tlearn: 1.6420867\ttotal: 2m 43s\tremaining: 10m 47s\n",
      "403:\tlearn: 1.6407207\ttotal: 2m 43s\tremaining: 10m 46s\n",
      "404:\tlearn: 1.6393589\ttotal: 2m 44s\tremaining: 10m 46s\n",
      "405:\tlearn: 1.6380136\ttotal: 2m 44s\tremaining: 10m 46s\n",
      "406:\tlearn: 1.6366383\ttotal: 2m 45s\tremaining: 10m 45s\n",
      "407:\tlearn: 1.6352785\ttotal: 2m 45s\tremaining: 10m 45s\n",
      "408:\tlearn: 1.6339261\ttotal: 2m 45s\tremaining: 10m 45s\n",
      "409:\tlearn: 1.6325147\ttotal: 2m 46s\tremaining: 10m 44s\n",
      "410:\tlearn: 1.6311097\ttotal: 2m 46s\tremaining: 10m 44s\n",
      "411:\tlearn: 1.6297796\ttotal: 2m 47s\tremaining: 10m 43s\n",
      "412:\tlearn: 1.6284506\ttotal: 2m 47s\tremaining: 10m 43s\n",
      "413:\tlearn: 1.6271247\ttotal: 2m 47s\tremaining: 10m 42s\n",
      "414:\tlearn: 1.6258186\ttotal: 2m 48s\tremaining: 10m 42s\n",
      "415:\tlearn: 1.6244742\ttotal: 2m 48s\tremaining: 10m 42s\n",
      "416:\tlearn: 1.6231579\ttotal: 2m 49s\tremaining: 10m 41s\n",
      "417:\tlearn: 1.6218495\ttotal: 2m 49s\tremaining: 10m 41s\n",
      "418:\tlearn: 1.6205758\ttotal: 2m 49s\tremaining: 10m 40s\n",
      "419:\tlearn: 1.6192567\ttotal: 2m 50s\tremaining: 10m 40s\n",
      "420:\tlearn: 1.6179602\ttotal: 2m 50s\tremaining: 10m 40s\n",
      "421:\tlearn: 1.6166618\ttotal: 2m 51s\tremaining: 10m 39s\n",
      "422:\tlearn: 1.6153398\ttotal: 2m 51s\tremaining: 10m 39s\n",
      "423:\tlearn: 1.6140476\ttotal: 2m 51s\tremaining: 10m 39s\n",
      "424:\tlearn: 1.6127592\ttotal: 2m 52s\tremaining: 10m 38s\n",
      "425:\tlearn: 1.6114569\ttotal: 2m 52s\tremaining: 10m 38s\n",
      "426:\tlearn: 1.6100997\ttotal: 2m 53s\tremaining: 10m 37s\n",
      "427:\tlearn: 1.6088050\ttotal: 2m 53s\tremaining: 10m 37s\n",
      "428:\tlearn: 1.6074470\ttotal: 2m 53s\tremaining: 10m 36s\n",
      "429:\tlearn: 1.6061787\ttotal: 2m 54s\tremaining: 10m 36s\n",
      "430:\tlearn: 1.6049104\ttotal: 2m 54s\tremaining: 10m 35s\n",
      "431:\tlearn: 1.6036367\ttotal: 2m 55s\tremaining: 10m 35s\n",
      "432:\tlearn: 1.6023694\ttotal: 2m 55s\tremaining: 10m 35s\n",
      "433:\tlearn: 1.6010956\ttotal: 2m 55s\tremaining: 10m 34s\n",
      "434:\tlearn: 1.5997857\ttotal: 2m 56s\tremaining: 10m 33s\n",
      "435:\tlearn: 1.5985138\ttotal: 2m 56s\tremaining: 10m 33s\n",
      "436:\tlearn: 1.5972536\ttotal: 2m 57s\tremaining: 10m 33s\n",
      "437:\tlearn: 1.5959976\ttotal: 2m 57s\tremaining: 10m 32s\n",
      "438:\tlearn: 1.5947235\ttotal: 2m 57s\tremaining: 10m 32s\n",
      "439:\tlearn: 1.5934527\ttotal: 2m 58s\tremaining: 10m 31s\n",
      "440:\tlearn: 1.5922203\ttotal: 2m 58s\tremaining: 10m 31s\n",
      "441:\tlearn: 1.5909457\ttotal: 2m 58s\tremaining: 10m 30s\n",
      "442:\tlearn: 1.5897170\ttotal: 2m 59s\tremaining: 10m 30s\n",
      "443:\tlearn: 1.5884520\ttotal: 2m 59s\tremaining: 10m 29s\n",
      "444:\tlearn: 1.5872361\ttotal: 3m\tremaining: 10m 29s\n",
      "445:\tlearn: 1.5859792\ttotal: 3m\tremaining: 10m 28s\n",
      "446:\tlearn: 1.5847640\ttotal: 3m\tremaining: 10m 28s\n",
      "447:\tlearn: 1.5835133\ttotal: 3m 1s\tremaining: 10m 27s\n",
      "448:\tlearn: 1.5823055\ttotal: 3m 1s\tremaining: 10m 27s\n",
      "449:\tlearn: 1.5810233\ttotal: 3m 2s\tremaining: 10m 27s\n",
      "450:\tlearn: 1.5798134\ttotal: 3m 2s\tremaining: 10m 26s\n",
      "451:\tlearn: 1.5786200\ttotal: 3m 2s\tremaining: 10m 26s\n",
      "452:\tlearn: 1.5774227\ttotal: 3m 3s\tremaining: 10m 26s\n",
      "453:\tlearn: 1.5762110\ttotal: 3m 3s\tremaining: 10m 25s\n",
      "454:\tlearn: 1.5749927\ttotal: 3m 4s\tremaining: 10m 25s\n",
      "455:\tlearn: 1.5737869\ttotal: 3m 4s\tremaining: 10m 25s\n",
      "456:\tlearn: 1.5725766\ttotal: 3m 5s\tremaining: 10m 25s\n",
      "457:\tlearn: 1.5713390\ttotal: 3m 5s\tremaining: 10m 24s\n",
      "458:\tlearn: 1.5701309\ttotal: 3m 6s\tremaining: 10m 24s\n",
      "459:\tlearn: 1.5689130\ttotal: 3m 6s\tremaining: 10m 24s\n",
      "460:\tlearn: 1.5677374\ttotal: 3m 6s\tremaining: 10m 23s\n",
      "461:\tlearn: 1.5665642\ttotal: 3m 7s\tremaining: 10m 23s\n",
      "462:\tlearn: 1.5653913\ttotal: 3m 7s\tremaining: 10m 23s\n",
      "463:\tlearn: 1.5641877\ttotal: 3m 8s\tremaining: 10m 23s\n",
      "464:\tlearn: 1.5629749\ttotal: 3m 8s\tremaining: 10m 23s\n",
      "465:\tlearn: 1.5617098\ttotal: 3m 9s\tremaining: 10m 23s\n",
      "466:\tlearn: 1.5605436\ttotal: 3m 9s\tremaining: 10m 22s\n",
      "467:\tlearn: 1.5593261\ttotal: 3m 10s\tremaining: 10m 22s\n",
      "468:\tlearn: 1.5581191\ttotal: 3m 10s\tremaining: 10m 22s\n",
      "469:\tlearn: 1.5569333\ttotal: 3m 10s\tremaining: 10m 21s\n",
      "470:\tlearn: 1.5557665\ttotal: 3m 11s\tremaining: 10m 21s\n",
      "471:\tlearn: 1.5546142\ttotal: 3m 11s\tremaining: 10m 20s\n",
      "472:\tlearn: 1.5534376\ttotal: 3m 12s\tremaining: 10m 20s\n",
      "473:\tlearn: 1.5523026\ttotal: 3m 12s\tremaining: 10m 20s\n",
      "474:\tlearn: 1.5511612\ttotal: 3m 13s\tremaining: 10m 19s\n",
      "475:\tlearn: 1.5499835\ttotal: 3m 13s\tremaining: 10m 19s\n",
      "476:\tlearn: 1.5487995\ttotal: 3m 13s\tremaining: 10m 18s\n",
      "477:\tlearn: 1.5476647\ttotal: 3m 14s\tremaining: 10m 18s\n",
      "478:\tlearn: 1.5465204\ttotal: 3m 14s\tremaining: 10m 18s\n",
      "479:\tlearn: 1.5453597\ttotal: 3m 15s\tremaining: 10m 17s\n",
      "480:\tlearn: 1.5442275\ttotal: 3m 15s\tremaining: 10m 17s\n",
      "481:\tlearn: 1.5430917\ttotal: 3m 15s\tremaining: 10m 16s\n",
      "482:\tlearn: 1.5419622\ttotal: 3m 16s\tremaining: 10m 16s\n",
      "483:\tlearn: 1.5408024\ttotal: 3m 16s\tremaining: 10m 16s\n",
      "484:\tlearn: 1.5396438\ttotal: 3m 17s\tremaining: 10m 15s\n",
      "485:\tlearn: 1.5384460\ttotal: 3m 17s\tremaining: 10m 15s\n",
      "486:\tlearn: 1.5373045\ttotal: 3m 17s\tremaining: 10m 14s\n",
      "487:\tlearn: 1.5361625\ttotal: 3m 18s\tremaining: 10m 14s\n",
      "488:\tlearn: 1.5350418\ttotal: 3m 18s\tremaining: 10m 14s\n",
      "489:\tlearn: 1.5338857\ttotal: 3m 19s\tremaining: 10m 13s\n",
      "490:\tlearn: 1.5327704\ttotal: 3m 19s\tremaining: 10m 13s\n",
      "491:\tlearn: 1.5316602\ttotal: 3m 19s\tremaining: 10m 12s\n",
      "492:\tlearn: 1.5305440\ttotal: 3m 20s\tremaining: 10m 12s\n",
      "493:\tlearn: 1.5293390\ttotal: 3m 20s\tremaining: 10m 12s\n",
      "494:\tlearn: 1.5282302\ttotal: 3m 21s\tremaining: 10m 11s\n",
      "495:\tlearn: 1.5271338\ttotal: 3m 21s\tremaining: 10m 11s\n",
      "496:\tlearn: 1.5260131\ttotal: 3m 22s\tremaining: 10m 11s\n",
      "497:\tlearn: 1.5249216\ttotal: 3m 22s\tremaining: 10m 10s\n",
      "498:\tlearn: 1.5238212\ttotal: 3m 22s\tremaining: 10m 10s\n",
      "499:\tlearn: 1.5227278\ttotal: 3m 23s\tremaining: 10m 10s\n",
      "500:\tlearn: 1.5216430\ttotal: 3m 23s\tremaining: 10m 9s\n",
      "501:\tlearn: 1.5204917\ttotal: 3m 24s\tremaining: 10m 9s\n",
      "502:\tlearn: 1.5193952\ttotal: 3m 24s\tremaining: 10m 9s\n",
      "503:\tlearn: 1.5183155\ttotal: 3m 25s\tremaining: 10m 8s\n",
      "504:\tlearn: 1.5172157\ttotal: 3m 25s\tremaining: 10m 8s\n",
      "505:\tlearn: 1.5161438\ttotal: 3m 25s\tremaining: 10m 7s\n",
      "506:\tlearn: 1.5150480\ttotal: 3m 26s\tremaining: 10m 7s\n",
      "507:\tlearn: 1.5139310\ttotal: 3m 26s\tremaining: 10m 7s\n",
      "508:\tlearn: 1.5128820\ttotal: 3m 27s\tremaining: 10m 6s\n",
      "509:\tlearn: 1.5118067\ttotal: 3m 27s\tremaining: 10m 6s\n",
      "510:\tlearn: 1.5107170\ttotal: 3m 27s\tremaining: 10m 5s\n",
      "511:\tlearn: 1.5096439\ttotal: 3m 28s\tremaining: 10m 5s\n",
      "512:\tlearn: 1.5085545\ttotal: 3m 28s\tremaining: 10m 4s\n",
      "513:\tlearn: 1.5075064\ttotal: 3m 29s\tremaining: 10m 4s\n",
      "514:\tlearn: 1.5064284\ttotal: 3m 29s\tremaining: 10m 4s\n",
      "515:\tlearn: 1.5053770\ttotal: 3m 29s\tremaining: 10m 3s\n",
      "516:\tlearn: 1.5043355\ttotal: 3m 30s\tremaining: 10m 3s\n",
      "517:\tlearn: 1.5032722\ttotal: 3m 30s\tremaining: 10m 2s\n",
      "518:\tlearn: 1.5022210\ttotal: 3m 31s\tremaining: 10m 2s\n",
      "519:\tlearn: 1.5011715\ttotal: 3m 31s\tremaining: 10m 2s\n",
      "520:\tlearn: 1.5001079\ttotal: 3m 31s\tremaining: 10m 1s\n",
      "521:\tlearn: 1.4990794\ttotal: 3m 32s\tremaining: 10m 1s\n",
      "522:\tlearn: 1.4980231\ttotal: 3m 32s\tremaining: 10m\n",
      "523:\tlearn: 1.4969625\ttotal: 3m 33s\tremaining: 10m\n",
      "524:\tlearn: 1.4959038\ttotal: 3m 33s\tremaining: 10m\n",
      "525:\tlearn: 1.4948987\ttotal: 3m 34s\tremaining: 9m 59s\n",
      "526:\tlearn: 1.4938730\ttotal: 3m 34s\tremaining: 9m 59s\n",
      "527:\tlearn: 1.4928315\ttotal: 3m 34s\tremaining: 9m 58s\n",
      "528:\tlearn: 1.4918056\ttotal: 3m 35s\tremaining: 9m 58s\n",
      "529:\tlearn: 1.4907779\ttotal: 3m 35s\tremaining: 9m 58s\n",
      "530:\tlearn: 1.4897501\ttotal: 3m 36s\tremaining: 9m 57s\n",
      "531:\tlearn: 1.4887351\ttotal: 3m 36s\tremaining: 9m 57s\n",
      "532:\tlearn: 1.4877058\ttotal: 3m 36s\tremaining: 9m 57s\n",
      "533:\tlearn: 1.4866977\ttotal: 3m 37s\tremaining: 9m 56s\n",
      "534:\tlearn: 1.4856879\ttotal: 3m 37s\tremaining: 9m 56s\n",
      "535:\tlearn: 1.4846718\ttotal: 3m 38s\tremaining: 9m 55s\n",
      "536:\tlearn: 1.4835976\ttotal: 3m 38s\tremaining: 9m 55s\n",
      "537:\tlearn: 1.4825201\ttotal: 3m 39s\tremaining: 9m 55s\n",
      "538:\tlearn: 1.4814807\ttotal: 3m 39s\tremaining: 9m 54s\n",
      "539:\tlearn: 1.4804517\ttotal: 3m 39s\tremaining: 9m 54s\n",
      "540:\tlearn: 1.4794372\ttotal: 3m 40s\tremaining: 9m 54s\n",
      "541:\tlearn: 1.4784188\ttotal: 3m 40s\tremaining: 9m 53s\n",
      "542:\tlearn: 1.4774029\ttotal: 3m 41s\tremaining: 9m 53s\n",
      "543:\tlearn: 1.4763271\ttotal: 3m 41s\tremaining: 9m 52s\n",
      "544:\tlearn: 1.4753227\ttotal: 3m 41s\tremaining: 9m 52s\n",
      "545:\tlearn: 1.4742514\ttotal: 3m 42s\tremaining: 9m 52s\n",
      "546:\tlearn: 1.4732454\ttotal: 3m 42s\tremaining: 9m 51s\n",
      "547:\tlearn: 1.4722554\ttotal: 3m 43s\tremaining: 9m 51s\n",
      "548:\tlearn: 1.4712703\ttotal: 3m 43s\tremaining: 9m 50s\n",
      "549:\tlearn: 1.4702386\ttotal: 3m 43s\tremaining: 9m 50s\n",
      "550:\tlearn: 1.4692584\ttotal: 3m 44s\tremaining: 9m 49s\n",
      "551:\tlearn: 1.4682278\ttotal: 3m 44s\tremaining: 9m 49s\n",
      "552:\tlearn: 1.4672366\ttotal: 3m 45s\tremaining: 9m 48s\n",
      "553:\tlearn: 1.4662241\ttotal: 3m 45s\tremaining: 9m 48s\n",
      "554:\tlearn: 1.4652451\ttotal: 3m 45s\tremaining: 9m 47s\n",
      "555:\tlearn: 1.4642548\ttotal: 3m 46s\tremaining: 9m 47s\n",
      "556:\tlearn: 1.4632882\ttotal: 3m 46s\tremaining: 9m 47s\n",
      "557:\tlearn: 1.4623153\ttotal: 3m 47s\tremaining: 9m 46s\n",
      "558:\tlearn: 1.4613641\ttotal: 3m 47s\tremaining: 9m 46s\n",
      "559:\tlearn: 1.4603911\ttotal: 3m 47s\tremaining: 9m 45s\n",
      "560:\tlearn: 1.4593926\ttotal: 3m 48s\tremaining: 9m 45s\n",
      "561:\tlearn: 1.4583715\ttotal: 3m 48s\tremaining: 9m 44s\n",
      "562:\tlearn: 1.4573888\ttotal: 3m 49s\tremaining: 9m 44s\n",
      "563:\tlearn: 1.4563957\ttotal: 3m 49s\tremaining: 9m 44s\n",
      "564:\tlearn: 1.4554053\ttotal: 3m 49s\tremaining: 9m 43s\n",
      "565:\tlearn: 1.4544234\ttotal: 3m 50s\tremaining: 9m 43s\n",
      "566:\tlearn: 1.4534549\ttotal: 3m 50s\tremaining: 9m 42s\n",
      "567:\tlearn: 1.4525027\ttotal: 3m 51s\tremaining: 9m 42s\n",
      "568:\tlearn: 1.4515521\ttotal: 3m 51s\tremaining: 9m 41s\n",
      "569:\tlearn: 1.4505913\ttotal: 3m 51s\tremaining: 9m 41s\n",
      "570:\tlearn: 1.4496043\ttotal: 3m 52s\tremaining: 9m 41s\n",
      "571:\tlearn: 1.4486707\ttotal: 3m 52s\tremaining: 9m 40s\n",
      "572:\tlearn: 1.4476922\ttotal: 3m 53s\tremaining: 9m 40s\n",
      "573:\tlearn: 1.4467539\ttotal: 3m 53s\tremaining: 9m 40s\n",
      "574:\tlearn: 1.4458313\ttotal: 3m 53s\tremaining: 9m 39s\n",
      "575:\tlearn: 1.4448430\ttotal: 3m 54s\tremaining: 9m 39s\n",
      "576:\tlearn: 1.4438902\ttotal: 3m 54s\tremaining: 9m 39s\n",
      "577:\tlearn: 1.4429010\ttotal: 3m 55s\tremaining: 9m 38s\n",
      "578:\tlearn: 1.4419759\ttotal: 3m 55s\tremaining: 9m 38s\n",
      "579:\tlearn: 1.4410013\ttotal: 3m 56s\tremaining: 9m 37s\n",
      "580:\tlearn: 1.4400486\ttotal: 3m 56s\tremaining: 9m 37s\n",
      "581:\tlearn: 1.4391175\ttotal: 3m 56s\tremaining: 9m 37s\n",
      "582:\tlearn: 1.4381790\ttotal: 3m 57s\tremaining: 9m 36s\n",
      "583:\tlearn: 1.4371995\ttotal: 3m 57s\tremaining: 9m 36s\n",
      "584:\tlearn: 1.4362699\ttotal: 3m 58s\tremaining: 9m 35s\n",
      "585:\tlearn: 1.4353142\ttotal: 3m 58s\tremaining: 9m 35s\n",
      "586:\tlearn: 1.4343886\ttotal: 3m 58s\tremaining: 9m 35s\n",
      "587:\tlearn: 1.4334717\ttotal: 3m 59s\tremaining: 9m 34s\n",
      "588:\tlearn: 1.4325636\ttotal: 3m 59s\tremaining: 9m 34s\n",
      "589:\tlearn: 1.4316516\ttotal: 4m\tremaining: 9m 33s\n",
      "590:\tlearn: 1.4307256\ttotal: 4m\tremaining: 9m 33s\n",
      "591:\tlearn: 1.4298160\ttotal: 4m\tremaining: 9m 32s\n",
      "592:\tlearn: 1.4288855\ttotal: 4m 1s\tremaining: 9m 32s\n",
      "593:\tlearn: 1.4279165\ttotal: 4m 1s\tremaining: 9m 31s\n",
      "594:\tlearn: 1.4270178\ttotal: 4m 2s\tremaining: 9m 31s\n",
      "595:\tlearn: 1.4260867\ttotal: 4m 2s\tremaining: 9m 31s\n",
      "596:\tlearn: 1.4251030\ttotal: 4m 2s\tremaining: 9m 30s\n",
      "597:\tlearn: 1.4241613\ttotal: 4m 3s\tremaining: 9m 30s\n",
      "598:\tlearn: 1.4232521\ttotal: 4m 3s\tremaining: 9m 29s\n",
      "599:\tlearn: 1.4223338\ttotal: 4m 3s\tremaining: 9m 29s\n",
      "600:\tlearn: 1.4214393\ttotal: 4m 4s\tremaining: 9m 28s\n",
      "601:\tlearn: 1.4205367\ttotal: 4m 4s\tremaining: 9m 28s\n",
      "602:\tlearn: 1.4196498\ttotal: 4m 5s\tremaining: 9m 28s\n",
      "603:\tlearn: 1.4186854\ttotal: 4m 5s\tremaining: 9m 27s\n",
      "604:\tlearn: 1.4177776\ttotal: 4m 6s\tremaining: 9m 27s\n",
      "605:\tlearn: 1.4168579\ttotal: 4m 6s\tremaining: 9m 26s\n",
      "606:\tlearn: 1.4159574\ttotal: 4m 6s\tremaining: 9m 26s\n",
      "607:\tlearn: 1.4150833\ttotal: 4m 7s\tremaining: 9m 25s\n",
      "608:\tlearn: 1.4142102\ttotal: 4m 7s\tremaining: 9m 25s\n",
      "609:\tlearn: 1.4133300\ttotal: 4m 7s\tremaining: 9m 25s\n",
      "610:\tlearn: 1.4124399\ttotal: 4m 8s\tremaining: 9m 24s\n",
      "611:\tlearn: 1.4115706\ttotal: 4m 8s\tremaining: 9m 24s\n",
      "612:\tlearn: 1.4106119\ttotal: 4m 9s\tremaining: 9m 23s\n",
      "613:\tlearn: 1.4097022\ttotal: 4m 9s\tremaining: 9m 23s\n",
      "614:\tlearn: 1.4087482\ttotal: 4m 10s\tremaining: 9m 23s\n",
      "615:\tlearn: 1.4078681\ttotal: 4m 10s\tremaining: 9m 22s\n",
      "616:\tlearn: 1.4070040\ttotal: 4m 10s\tremaining: 9m 22s\n",
      "617:\tlearn: 1.4061431\ttotal: 4m 11s\tremaining: 9m 21s\n",
      "618:\tlearn: 1.4052364\ttotal: 4m 11s\tremaining: 9m 21s\n",
      "619:\tlearn: 1.4043804\ttotal: 4m 12s\tremaining: 9m 21s\n",
      "620:\tlearn: 1.4035059\ttotal: 4m 12s\tremaining: 9m 20s\n",
      "621:\tlearn: 1.4026474\ttotal: 4m 12s\tremaining: 9m 20s\n",
      "622:\tlearn: 1.4017684\ttotal: 4m 13s\tremaining: 9m 20s\n",
      "623:\tlearn: 1.4009157\ttotal: 4m 13s\tremaining: 9m 19s\n",
      "624:\tlearn: 1.4000549\ttotal: 4m 14s\tremaining: 9m 19s\n",
      "625:\tlearn: 1.3991944\ttotal: 4m 14s\tremaining: 9m 18s\n",
      "626:\tlearn: 1.3982777\ttotal: 4m 14s\tremaining: 9m 18s\n",
      "627:\tlearn: 1.3974198\ttotal: 4m 15s\tremaining: 9m 17s\n",
      "628:\tlearn: 1.3965406\ttotal: 4m 15s\tremaining: 9m 17s\n",
      "629:\tlearn: 1.3956780\ttotal: 4m 16s\tremaining: 9m 17s\n",
      "630:\tlearn: 1.3947510\ttotal: 4m 16s\tremaining: 9m 16s\n",
      "631:\tlearn: 1.3938772\ttotal: 4m 16s\tremaining: 9m 16s\n",
      "632:\tlearn: 1.3930105\ttotal: 4m 17s\tremaining: 9m 15s\n",
      "633:\tlearn: 1.3921717\ttotal: 4m 17s\tremaining: 9m 15s\n",
      "634:\tlearn: 1.3913236\ttotal: 4m 18s\tremaining: 9m 14s\n",
      "635:\tlearn: 1.3904903\ttotal: 4m 18s\tremaining: 9m 14s\n",
      "636:\tlearn: 1.3896525\ttotal: 4m 19s\tremaining: 9m 14s\n",
      "637:\tlearn: 1.3887941\ttotal: 4m 19s\tremaining: 9m 13s\n",
      "638:\tlearn: 1.3879199\ttotal: 4m 19s\tremaining: 9m 13s\n",
      "639:\tlearn: 1.3870744\ttotal: 4m 20s\tremaining: 9m 12s\n",
      "640:\tlearn: 1.3862425\ttotal: 4m 20s\tremaining: 9m 12s\n",
      "641:\tlearn: 1.3854081\ttotal: 4m 21s\tremaining: 9m 12s\n",
      "642:\tlearn: 1.3845869\ttotal: 4m 21s\tremaining: 9m 11s\n",
      "643:\tlearn: 1.3837715\ttotal: 4m 21s\tremaining: 9m 11s\n"
     ]
    }
   ],
   "source": [
    "#Catboost\n",
    "import catboost as cb\n",
    "cb_clf = cb.CatBoostClassifier(n_estimators = 2000,\n",
    "                               learning_rate = 0.001)\n",
    "cb_clf.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_pred = cb_model.predict_proba(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "score = metrics.log_loss(y_valid, cb_pred)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38302130295075365"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(booster = 'gbtree', \n",
    "                    n_jobs = -1, \n",
    "                    objective = 'multi:softprob', \n",
    "                    eval_metric = 'mlogloss',\n",
    "                    estimator = 1000,\n",
    "                    )\n",
    "xgb.fit(X_train,y_train)\n",
    "xgb_pred = xgb.predict_proba(X_valid)\n",
    "score = metrics.log_loss(y_valid, xgb_pred)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred =rf_3000.predict_proba(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(data=y_pred, columns=sample_submission.columns, index=sample_submission.index)\n",
    "submission.to_csv('submission.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
